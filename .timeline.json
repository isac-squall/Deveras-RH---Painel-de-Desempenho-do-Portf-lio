{
  "timeline_index": 23,
  "planner": {
    "Team Leader": {
      "1": {
        "task_id": "1",
        "dependent_task_ids": [],
        "instruction": "Analyze, clean and process the Excel file '/data/chats/igk4wd/workspace/uploads/Teste Power BI - Deveras RH.xlsx'. Equalize the data bases (format, columns, colors, etc.), analyze portfolios (total and individual) for each operating unit, and identify any inconsistencies in the spreadsheet data. Save the processed data to '/data/chats/igk4wd/workspace/processed_data.csv' for dashboard creation.",
        "task_type": "general",
        "code": "",
        "result": "",
        "is_success": false,
        "is_finished": true,
        "assignee": "David",
        "start_at": 1748200186.8091755,
        "end_at": 1748200799.760214
      },
      "current_task_id": "2",
      "2": {
        "task_id": "2",
        "dependent_task_ids": [
          "1"
        ],
        "instruction": "Create a dashboard using Streamlit that shows portfolio performance based on the processed data at '/data/chats/igk4wd/workspace/processed_data.csv'. The dashboard should visualize the performance metrics for portfolios across different operating units.",
        "task_type": "general",
        "code": "",
        "result": "",
        "is_success": false,
        "is_finished": false,
        "assignee": "Alex",
        "start_at": 1748200799.76024
      }
    },
    "Data Analyst": {
      "1": {
        "task_id": "1",
        "dependent_task_ids": [],
        "instruction": "Examine the Excel file structure and content to understand the data format and identify any initial inconsistencies",
        "task_type": "eda",
        "code": "\nimport pandas as pd\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Path to the file\nfile_path = '/data/chats/igk4wd/workspace/uploads/Teste Power BI - Deveras RH.xlsx'\n\n# List all sheets in the Excel file\nexcel_file = pd.ExcelFile(file_path)\nsheet_names = excel_file.sheet_names\nprint(\"Excel sheets:\", sheet_names)\n\n# Create a dictionary to store dataframes from each sheet\ndfs = {}\n\n# Read each sheet and store in the dictionary\nfor sheet in sheet_names:\n    dfs[sheet] = pd.read_excel(file_path, sheet_name=sheet)\n    print(f\"\\n--- Sheet: {sheet} ---\")\n    print(f\"Shape: {dfs[sheet].shape}\")\n    print(\"Column names:\", list(dfs[sheet].columns))\n    print(\"Data types:\\n\", dfs[sheet].dtypes)\n    \n    # Display first few rows\n    print(\"First 3 rows:\\n\", dfs[sheet].head(3))\n    \n    # Check for missing values\n    missing_values = dfs[sheet].isnull().sum()\n    if missing_values.sum() > 0:\n        print(\"Missing values:\\n\", missing_values[missing_values > 0])\n    else:\n        print(\"No missing values found.\")\n    \n    # Check for duplicates\n    duplicates = dfs[sheet].duplicated().sum()\n    print(f\"Number of duplicate rows: {duplicates}\")\n# Now let's continue our analysis by focusing on the remaining sheets\n# and convert the VALOR column to numeric for analysis\n\n# Let's look at the remaining sheets in more depth\nlocation_sheets = ['Alphaville', 'BH', 'SJC', 'PAULINIA', 'CAMPINAS']\n\n# Dictionary to store cleaned data frames\ncleaned_dfs = {}\n\nfor sheet in location_sheets:\n    if sheet in dfs:\n        print(f\"\\n--- Detailed analysis of {sheet} sheet ---\")\n        \n        # Convert VALOR column to numeric (removing any currency symbols, etc.)\n        if 'VALOR' in dfs[sheet].columns:\n            # First, check the current format\n            print(f\"VALOR column sample values: {dfs[sheet]['VALOR'].head(3).tolist()}\")\n            \n            # Try to convert to numeric, coercing errors to NaN\n            dfs[sheet]['VALOR_NUMERIC'] = pd.to_numeric(\n                dfs[sheet]['VALOR'].str.replace('R\\$', '').str.replace('.', '').str.replace(',', '.'),\n                errors='coerce'\n            )\n            \n            # Check if the conversion worked\n            print(f\"Converted VALOR sample: {dfs[sheet]['VALOR_NUMERIC'].head(3).tolist()}\")\n            print(f\"NaN values after conversion: {dfs[sheet]['VALOR_NUMERIC'].isna().sum()}\")\n        \n        # Check for data consistency between sheets\n        print(f\"Column count: {dfs[sheet].shape[1]}\")\n        \n        # Analyze key statistics\n        numeric_columns = dfs[sheet].select_dtypes(include=[np.number])\n        if not numeric_columns.empty:\n            print(\"\\nNumeric columns statistics:\")\n            print(numeric_columns.describe().T)\n        \n        # Analyze categorical columns\n        categorical_columns = dfs[sheet].select_dtypes(include=['object'])\n        if not categorical_columns.empty:\n            print(\"\\nCategorical columns unique values count:\")\n            for col in categorical_columns.columns[:5]:  # Limiting to first 5 columns to avoid too much output\n                print(f\"{col}: {categorical_columns[col].nunique()} unique values\")\n                print(f\"Top 3 values: {categorical_columns[col].value_counts().nlargest(3)}\")\n        \n        # Check date columns for temporal range\n        date_columns = dfs[sheet].select_dtypes(include=['datetime64'])\n        if not date_columns.empty:\n            print(\"\\nDate ranges:\")\n            for col in date_columns.columns:\n                min_date = date_columns[col].min()\n                max_date = date_columns[col].max()\n                print(f\"{col}: {min_date} to {max_date}\")\n        \n        # Store the dataframe with converted values\n        cleaned_dfs[sheet] = dfs[sheet]\n\n# Check for inconsistencies between location sheets\nprint(\"\\n--- Checking inconsistencies between sheets ---\")\nif len(location_sheets) > 1 and all(sheet in dfs for sheet in location_sheets):\n    # Check if all sheets have the same columns\n    all_columns = [set(dfs[sheet].columns) for sheet in location_sheets]\n    common_columns = set.intersection(*all_columns)\n    \n    print(f\"Common columns across all location sheets: {len(common_columns)}\")\n    \n    # Check for differences in columns\n    for sheet in location_sheets:\n        diff_columns = set(dfs[sheet].columns) - common_columns\n        if diff_columns:\n            print(f\"{sheet} has {len(diff_columns)} unique columns: {diff_columns}\")\n# Fix the VALOR conversion issue and complete the analysis of all sheets\nimport re\n\n# Function to properly convert VALOR to numeric\ndef convert_to_numeric(value):\n    if pd.isna(value):\n        return np.nan\n    if isinstance(value, (int, float)):\n        return value\n    \n    # Remove non-numeric characters except for decimal separator\n    # First, standardize the value format\n    value_str = str(value)\n    # Remove currency symbols, spaces, etc.\n    cleaned = re.sub(r'[^\\d,.]', '', value_str)\n    # Convert Brazilian/European format (1.234,56) to US format (1234.56)\n    if ',' in cleaned and '.' in cleaned:\n        cleaned = cleaned.replace('.', '').replace(',', '.')\n    elif ',' in cleaned:\n        cleaned = cleaned.replace(',', '.')\n    \n    try:\n        return float(cleaned)\n    except:\n        return np.nan\n\n# Create a comprehensive comparison of all location sheets\nlocation_sheets = ['Alphaville', 'BH', 'SJC', 'PAULINIA', 'CAMPINAS']\nsummary_data = {}\n\n# Process each sheet\nfor sheet in location_sheets:\n    if sheet in dfs:\n        df = dfs[sheet].copy()\n        \n        print(f\"\\n--- Processing {sheet} sheet ---\")\n        \n        # Convert VALOR to numeric\n        if 'VALOR' in df.columns:\n            # Apply the conversion function\n            df['VALOR_NUMERIC'] = df['VALOR'].apply(convert_to_numeric)\n            print(f\"Converted VALOR sample: {df['VALOR_NUMERIC'].head(3).tolist()}\")\n            print(f\"NaN values after conversion: {df['VALOR_NUMERIC'].isna().sum()}\")\n            \n            # Basic statistics on VALOR\n            if df['VALOR_NUMERIC'].notna().any():\n                print(f\"VALOR statistics: Min={df['VALOR_NUMERIC'].min()}, Max={df['VALOR_NUMERIC'].max()}, Mean={df['VALOR_NUMERIC'].mean()}\")\n        \n        # Check for inconsistent column names (case, spacing, etc.)\n        column_names = df.columns.tolist()\n        cleaned_column_names = [col.strip().upper() for col in column_names]\n        if len(set(cleaned_column_names)) != len(column_names):\n            print(\"Warning: Potential inconsistent column naming (case or spacing)\")\n            duplicates = [col for col in cleaned_column_names if cleaned_column_names.count(col) > 1]\n            if duplicates:\n                print(f\"Duplicate columns after normalization: {set(duplicates)}\")\n        \n        # Check for inconsistencies in categorical data\n        if 'UNIDADE' in df.columns:\n            print(f\"UNIDADE values in {sheet}: {df['UNIDADE'].unique()}\")\n        \n        # Store sheet-specific data for summary\n        summary_data[sheet] = {\n            'row_count': df.shape[0],\n            'column_count': df.shape[1],\n            'missing_values': df.isnull().sum().sum(),\n            'date_range': None\n        }\n        \n        # Get date range if available\n        date_cols = df.select_dtypes(include=['datetime64']).columns\n        if len(date_cols) > 0:\n            min_date = df[date_cols].min().min()\n            max_date = df[date_cols].max().max()\n            summary_data[sheet]['date_range'] = (min_date, max_date)\n        \n        # Store the cleaned dataframe\n        cleaned_dfs[sheet] = df\n\n# Summarize the findings\nprint(\"\\n--- Summary of Data Analysis ---\")\nprint(f\"Total sheets analyzed: {len(location_sheets)}\")\nfor sheet, data in summary_data.items():\n    print(f\"\\n{sheet}:\")\n    print(f\"  - Records: {data['row_count']}\")\n    print(f\"  - Fields: {data['column_count']}\")\n    print(f\"  - Missing values: {data['missing_values']}\")\n    if data['date_range']:\n        print(f\"  - Date range: {data['date_range'][0]} to {data['date_range'][1]}\")\n\n# Identify structure inconsistencies across sheets\nprint(\"\\n--- Structural Inconsistencies ---\")\nall_columns = [set(dfs[sheet].columns) for sheet in location_sheets if sheet in dfs]\nif all_columns:\n    common_columns = set.intersection(*all_columns)\n    all_columns_flat = set().union(*all_columns)\n    \n    print(f\"Total unique columns across all sheets: {len(all_columns_flat)}\")\n    print(f\"Common columns across all sheets: {len(common_columns)}\")\n    \n    # Find columns unique to specific sheets\n    for i, sheet in enumerate(location_sheets):\n        if sheet in dfs:\n            unique_cols = set(dfs[sheet].columns) - common_columns\n            if unique_cols:\n                print(f\"{sheet} has {len(unique_cols)} unique columns: {unique_cols}\")\n    \n    # Check type inconsistencies in common columns\n    type_inconsistencies = []\n    for col in common_columns:\n        col_types = {}\n        for sheet in location_sheets:\n            if sheet in dfs and col in dfs[sheet].columns:\n                col_type = str(dfs[sheet][col].dtype)\n                if col_type not in col_types:\n                    col_types[col_type] = []\n                col_types[col_type].append(sheet)\n        \n        if len(col_types) > 1:\n            type_inconsistencies.append((col, col_types))\n    \n    if type_inconsistencies:\n        print(\"\\nType inconsistencies in common columns:\")\n        for col, types in type_inconsistencies:\n            print(f\"Column '{col}' has different types:\")\n            for type_name, sheets in types.items():\n                print(f\"  - {type_name}: in sheets {', '.join(sheets)}\")\n# Standardize and clean the datasets across all sheets\n# 1. Create a mapping for column name standardization\n# 2. Standardize data types\n# 3. Consolidate all data into a single clean dataset\n\n# Define standard column names mapping\n# This will standardize names across sheets and fix inconsistencies\ncolumn_mapping = {\n    # Contract and client info\n    'Nº CONTRATO (SISTEMA GERA)': 'CONTRATO_ID',\n    'Nº CONTRATO (SISTEMA GERA)2': 'CONTRATO_ID',\n    'CÓD CLIENTE': 'CLIENTE_ID',\n    'PACIENTYES': 'PACIENTE_NOME',\n    'PACIENTE 2': 'PACIENTE_NOME',\n    'CLIENTES': 'PACIENTE_NOME',\n    'DATA DO CONTRATO': 'DATA_CONTRATO',\n    \n    # Location and operational info\n    'UNIDADE': 'UNIDADE',\n    'OPERADORA': 'OPERADORA',\n    'STATUS': 'STATUS',\n    \n    # Financial data\n    ' NF': 'NF',\n    'VALOR': 'VALOR_BRUTO',\n    'VALOR_NUMERIC': 'VALOR',\n    'VALOR FINANCIADO': 'VALOR_FINANCIADO',\n    'VALOR PAGAMENTO CLÍNICA': 'VALOR_PAGAMENTO_CLINICA',\n    'VALOR PAGO': 'VALOR_PAGO',\n    'VALOR GLOSADO': 'VALOR_GLOSADO',\n    'VALOR PAGAMENTO FACTORING': 'VALOR_PAGAMENTO_FACTORING',\n    'CONTAS A RECEBER': 'CONTAS_A_RECEBER',\n    \n    # Date information\n    'DATA EMISSÃO NF': 'DATA_EMISSAO_NF',\n    'COMPETÊNCIA': 'COMPETENCIA',\n    'DATA FINANCIAMENTO': 'DATA_FINANCIAMENTO',\n    'DATA PAGAMENTO CLÍNICA': 'DATA_PAGAMENTO_CLINICA',\n    'DATA PROTOCOLO': 'DATA_PROTOCOLO',\n    'DATA PROTOCOLO ': 'DATA_PROTOCOLO',\n    'DATA DE VENCIMENTO': 'DATA_VENCIMENTO',\n    'DATA DO PAGAMENTO REEMBOLSO': 'DATA_PAGAMENTO_REEMBOLSO',\n    'DATA DO PAGAMENTO FACTORING': 'DATA_PAGAMENTO_FACTORING',\n    'DATA DA LIGAÇÃO': 'DATA_LIGACAO',\n    'DATA DA RECLAMAÇÃO': 'DATA_RECLAMACAO',\n    'DATA DA NIP': 'DATA_NIP',\n    \n    # Sessions and service information\n    'Nº SESSÕES': 'NUM_SESSOES',\n    'QUINZENA': 'QUINZENA',\n    \n    # Status fields\n    'STATUS PRÉVIO DA CONTA': 'STATUS_PREVIO_CONTA',\n    'STATUS ANÁLISE DE CRÉDITO': 'STATUS_ANALISE_CREDITO',\n    'STATUS JURÍDICO CLIENTE': 'STATUS_JURIDICO_CLIENTE',\n    'STATUS CONTAS A RECEBER': 'STATUS_CONTAS_RECEBER',\n    'STATUS FINAL DA CONTA': 'STATUS_FINAL_CONTA',\n    \n    # Other fields\n    'SOLICITANTE': 'SOLICITANTE',\n    'CONTRATO ASSINADO': 'CONTRATO_ASSINADO',\n    'FINANCIADO': 'FINANCIADO',\n    'PAGAMENTO CLINICA': 'PAGAMENTO_CLINICA',\n    'TRANSMISSÃO DA CONTA': 'TRANSMISSAO_CONTA',\n    'LIGAÇÃO': 'LIGACAO',\n    'RECLAMAÇÃO CHAT': 'RECLAMACAO_CHAT',\n    'NIP': 'NIP',\n    'ETAPAS DA JUDICILIAZAÇÃO': 'ETAPAS_JUDICIALIZACAO',\n    'OBSERVAÇÕES': 'OBSERVACOES',\n    '1ª APROVAÇÃO CLÍNICA GARANTIDORA': 'APROVACAO_CLINICA_GARANTIDORA',\n    '2ª APROVAÇÃO FACTORING': 'APROVACAO_FACTORING',\n    'USUÁRIO': 'USUARIO'\n}\n\n# Function to standardize column names\ndef standardize_column_names(df):\n    # Rename columns based on mapping\n    df_cols = df.columns.tolist()\n    rename_dict = {}\n    \n    for col in df_cols:\n        if col in column_mapping:\n            rename_dict[col] = column_mapping[col]\n    \n    return df.rename(columns=rename_dict)\n\n# Function to standardize data types\ndef standardize_data_types(df):\n    # Date columns to convert - some may not exist in all sheets\n    date_columns = [\n        'DATA_EMISSAO_NF', 'COMPETENCIA', 'DATA_FINANCIAMENTO', \n        'DATA_PAGAMENTO_CLINICA', 'DATA_PROTOCOLO', 'DATA_VENCIMENTO',\n        'DATA_PAGAMENTO_REEMBOLSO', 'DATA_PAGAMENTO_FACTORING',\n        'DATA_LIGACAO', 'DATA_RECLAMACAO', 'DATA_NIP', 'DATA_CONTRATO'\n    ]\n    \n    # Numeric columns to convert\n    numeric_columns = [\n        'VALOR', 'VALOR_FINANCIADO', 'VALOR_PAGAMENTO_CLINICA', \n        'VALOR_PAGO', 'VALOR_GLOSADO', 'VALOR_PAGAMENTO_FACTORING', \n        'CONTAS_A_RECEBER', 'NF', 'NUM_SESSOES', 'QUINZENA'\n    ]\n    \n    # Convert date columns to datetime\n    for col in date_columns:\n        if col in df.columns:\n            try:\n                # Handle different date formats\n                df[col] = pd.to_datetime(df[col], errors='coerce')\n                \n                # Handle very old dates (likely errors)\n                # Replace dates before 2000 with NaT\n                if df[col].dt.year.min() < 2000:\n                    df.loc[df[col].dt.year < 2000, col] = pd.NaT\n            except:\n                print(f\"Could not convert {col} to datetime in current dataframe\")\n    \n    # Convert numeric columns to float\n    for col in numeric_columns:\n        if col in df.columns:\n            # If the column already has numeric values, keep them\n            if pd.api.types.is_numeric_dtype(df[col]):\n                continue\n                \n            try:\n                # If not already numeric, try to convert to numeric\n                if col == 'VALOR' and 'VALOR_BRUTO' in df.columns:\n                    # Use the already converted VALOR_NUMERIC values\n                    pass\n                else:\n                    df[col] = pd.to_numeric(df[col], errors='coerce')\n            except:\n                print(f\"Could not convert {col} to numeric in current dataframe\")\n    \n    # Convert ID columns to string to ensure consistent handling\n    id_columns = ['CONTRATO_ID', 'CLIENTE_ID', 'NF']\n    for col in id_columns:\n        if col in df.columns:\n            df[col] = df[col].astype(str)\n    \n    return df\n\n# Function to standardize categorical values\ndef standardize_categorical_values(df):\n    # Standardize UNIDADE values\n    if 'UNIDADE' in df.columns:\n        # Standardize case and remove extra spaces\n        df['UNIDADE'] = df['UNIDADE'].str.strip().str.upper()\n        \n        # Map variations to standard names\n        unit_mapping = {\n            'ALPHA': 'ALPHAVILLE',\n            'alpha': 'ALPHAVILLE',\n            'BELO HORIZONTE': 'BH',\n            'BSB': 'BRASILIA',\n            'SJC': 'SAO JOSE DOS CAMPOS'\n        }\n        \n        df['UNIDADE'] = df['UNIDADE'].replace(unit_mapping)\n    \n    # Standardize STATUS values\n    if 'STATUS' in df.columns:\n        df['STATUS'] = df['STATUS'].str.strip().str.upper()\n    \n    return df\n\n# Function to add source sheet information\ndef add_sheet_info(df, sheet_name):\n    df['FONTE_PLANILHA'] = sheet_name\n    return df\n\n# Process all location sheets and combine into a single dataframe\nlocation_sheets = ['Alphaville', 'BH', 'SJC', 'PAULINIA', 'CAMPINAS']\nall_data = []\n\nfor sheet in location_sheets:\n    if sheet in dfs:\n        print(f\"Standardizing {sheet} sheet...\")\n        \n        # Make a copy to avoid modifying original data\n        df = dfs[sheet].copy()\n        \n        # Apply standardization steps\n        df = standardize_column_names(df)\n        df = standardize_data_types(df)\n        df = standardize_categorical_values(df)\n        df = add_sheet_info(df, sheet)\n        \n        # Append to list for later concatenation\n        all_data.append(df)\n        \n        print(f\"  Processed {df.shape[0]} rows with {df.shape[1]} columns\")\n\n# Combine all data into a single dataframe\nif all_data:\n    combined_df = pd.concat(all_data, ignore_index=True)\n    \n    # Final cleanup\n    # 1. Remove duplicate rows\n    initial_rows = combined_df.shape[0]\n    combined_df = combined_df.drop_duplicates()\n    duplicate_rows = initial_rows - combined_df.shape[0]\n    \n    # 2. Check if client IDs are consistent\n    if 'CLIENTE_ID' in combined_df.columns and 'PACIENTE_NOME' in combined_df.columns:\n        client_id_mapping = combined_df.groupby('CLIENTE_ID')['PACIENTE_NOME'].nunique()\n        inconsistent_ids = client_id_mapping[client_id_mapping > 1]\n        \n        if not inconsistent_ids.empty:\n            print(\"\\nInconsistent client IDs (multiple names for the same ID):\")\n            for client_id in inconsistent_ids.index:\n                print(f\"  Client ID {client_id} has {inconsistent_ids[client_id]} different names:\")\n                names = combined_df[combined_df['CLIENTE_ID'] == client_id]['PACIENTE_NOME'].unique()\n                for name in names:\n                    print(f\"    - {name}\")\n    \n    # Summary of standardized data\n    print(\"\\n=== STANDARDIZED DATA SUMMARY ===\")\n    print(f\"Total records: {combined_df.shape[0]}\")\n    print(f\"Duplicate rows removed: {duplicate_rows}\")\n    print(f\"Final columns: {combined_df.shape[1]}\")\n    print(f\"Column names: {combined_df.columns.tolist()}\")\n    \n    # Check for remaining missing values\n    missing_values = combined_df.isnull().sum()\n    significant_missing = missing_values[missing_values > combined_df.shape[0] * 0.2]  # More than 20% missing\n    if not significant_missing.empty:\n        print(\"\\nColumns with significant missing values (>20%):\")\n        for col, count in significant_missing.items():\n            print(f\"  {col}: {count} missing values ({count/combined_df.shape[0]:.1%})\")\n    \n    # Save the standardized data to CSV\n    output_path = '/data/chats/igk4wd/workspace/processed_data.csv'\n    combined_df.to_csv(output_path, index=False)\n    print(f\"\\nStandardized data saved to {output_path}\")\n    \n    # Basic portfolio analysis\n    print(\"\\n=== PORTFOLIO ANALYSIS ===\")\n    if 'UNIDADE' in combined_df.columns:\n        unit_counts = combined_df['UNIDADE'].value_counts()\n        print(\"Records by operating unit:\")\n        for unit, count in unit_counts.items():\n            print(f\"  {unit}: {count} records\")\n    \n    if 'VALOR' in combined_df.columns:\n        # Filter out extreme values for analysis\n        value_df = combined_df[combined_df['VALOR'] < combined_df['VALOR'].quantile(0.99)]\n        \n        print(\"\\nFinancial summary by unit:\")\n        for unit in combined_df['UNIDADE'].unique():\n            unit_data = value_df[value_df['UNIDADE'] == unit]\n            if not unit_data.empty and 'VALOR' in unit_data.columns:\n                total = unit_data['VALOR'].sum()\n                mean = unit_data['VALOR'].mean()\n                count = unit_data['VALOR'].count()\n                print(f\"  {unit}: {count} transactions, Total: R$ {total:,.2f}, Average: R$ {mean:,.2f}\")\nelse:\n    print(\"No data to standardize!\")\n# Further analyze the portfolio performance by operating unit\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.ticker import FuncFormatter\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\n# Load the processed data\nprocessed_data_path = '/data/chats/igk4wd/workspace/processed_data.csv'\ndf = pd.read_csv(processed_data_path)\n\n# Convert date columns to datetime\ndate_columns = [col for col in df.columns if col.startswith('DATA_')]\nfor col in date_columns:\n    df[col] = pd.to_datetime(df[col], errors='coerce')\n\n# Add month and year columns for time-based analysis\nif 'DATA_EMISSAO_NF' in df.columns:\n    df['MES'] = df['DATA_EMISSAO_NF'].dt.month\n    df['ANO'] = df['DATA_EMISSAO_NF'].dt.year\n    df['MES_ANO'] = df['DATA_EMISSAO_NF'].dt.strftime('%m-%Y')\n\n# Set plotting style\nsns.set(style=\"whitegrid\")\nplt.rcParams.update({'font.size': 12})\n\n# Create a folder for visualizations\nimport os\nviz_path = '/data/chats/igk4wd/workspace/vizualizacoes'\nos.makedirs(viz_path, exist_ok=True)\n\n# Filter out extreme values for visualization\ndf_viz = df.copy()\n# Filter out extreme values (above 99th percentile) for better visualization\nif 'VALOR' in df_viz.columns:\n    valor_threshold = df_viz['VALOR'].quantile(0.99)\n    df_viz = df_viz[df_viz['VALOR'] <= valor_threshold]\n\n# 1. Portfolio Analysis by Unit - Total Value\nplt.figure(figsize=(12, 8))\nunit_values = df_viz.groupby('UNIDADE')['VALOR'].sum().sort_values(ascending=False)\n\n# Bar plot for total values by unit\nax = sns.barplot(x=unit_values.index, y=unit_values.values, palette='viridis')\nplt.title('Total Portfolio Value by Operating Unit', fontsize=16)\nplt.xlabel('Operating Unit', fontsize=14)\nplt.ylabel('Total Value (R$)', fontsize=14)\nplt.xticks(rotation=45)\n\n# Add value labels on top of bars\nfor i, v in enumerate(unit_values):\n    ax.text(i, v + (unit_values.max() * 0.02), f'R$ {v:,.0f}', ha='center', fontsize=12)\n\nplt.tight_layout()\nplt.savefig(f'{viz_path}/total_portfolio_by_unit.png', dpi=300)\nplt.close()\n\n# 2. Average Contract Value by Unit\nplt.figure(figsize=(12, 8))\navg_values = df_viz.groupby('UNIDADE')['VALOR'].mean().sort_values(ascending=False)\n\n# Bar plot for average values by unit\nax = sns.barplot(x=avg_values.index, y=avg_values.values, palette='rocket')\nplt.title('Average Contract Value by Operating Unit', fontsize=16)\nplt.xlabel('Operating Unit', fontsize=14)\nplt.ylabel('Average Value (R$)', fontsize=14)\nplt.xticks(rotation=45)\n\n# Add value labels on top of bars\nfor i, v in enumerate(avg_values):\n    ax.text(i, v + (avg_values.max() * 0.02), f'R$ {v:,.0f}', ha='center', fontsize=12)\n\nplt.tight_layout()\nplt.savefig(f'{viz_path}/avg_contract_by_unit.png', dpi=300)\nplt.close()\n\n# 3. Contract Count by Operating Unit\nplt.figure(figsize=(12, 8))\ncontract_counts = df.groupby('UNIDADE').size().sort_values(ascending=False)\n\n# Bar plot for contract counts by unit\nax = sns.barplot(x=contract_counts.index, y=contract_counts.values, palette='mako')\nplt.title('Number of Contracts by Operating Unit', fontsize=16)\nplt.xlabel('Operating Unit', fontsize=14)\nplt.ylabel('Number of Contracts', fontsize=14)\nplt.xticks(rotation=45)\n\n# Add value labels on top of bars\nfor i, v in enumerate(contract_counts):\n    ax.text(i, v + (contract_counts.max() * 0.02), f'{v:,}', ha='center', fontsize=12)\n\nplt.tight_layout()\nplt.savefig(f'{viz_path}/contract_count_by_unit.png', dpi=300)\nplt.close()\n\n# 4. Time Series Analysis - Monthly Portfolio Value by Unit\n# Filter data from the last 12 months for clearer visualization\nif 'DATA_EMISSAO_NF' in df.columns:\n    # Get last 12 months of data with valid dates and values\n    df_recent = df[df['DATA_EMISSAO_NF'].notna() & df['VALOR'].notna()].copy()\n    \n    if not df_recent.empty:\n        # Sort by date\n        df_recent = df_recent.sort_values('DATA_EMISSAO_NF')\n        \n        # Group by month-year and unit\n        monthly_values = df_recent.groupby(['MES_ANO', 'UNIDADE'])['VALOR'].sum().reset_index()\n        \n        # Create pivot table for easier plotting\n        pivot_monthly = monthly_values.pivot(index='MES_ANO', columns='UNIDADE', values='VALOR')\n        \n        # Sort chronologically\n        try:\n            # Convert index to datetime for proper sorting\n            pivot_monthly.index = pd.to_datetime(pivot_monthly.index, format='%m-%Y')\n            pivot_monthly = pivot_monthly.sort_index()\n            # Convert back to string format for display\n            pivot_monthly.index = pivot_monthly.index.strftime('%m-%Y')\n        except:\n            pass\n        \n        # Plot time series\n        plt.figure(figsize=(14, 10))\n        pivot_monthly.plot(kind='line', marker='o', linewidth=2, figsize=(14, 10))\n        plt.title('Monthly Portfolio Value by Operating Unit', fontsize=16)\n        plt.xlabel('Month-Year', fontsize=14)\n        plt.ylabel('Total Value (R$)', fontsize=14)\n        plt.grid(True, linestyle='--', alpha=0.7)\n        plt.legend(title='Operating Unit', fontsize=12, title_fontsize=14)\n        plt.xticks(rotation=45)\n        \n        # Format y-axis to show currency\n        def currency_formatter(x, pos):\n            return f'R$ {x:,.0f}'\n        plt.gca().yaxis.set_major_formatter(FuncFormatter(currency_formatter))\n        \n        plt.tight_layout()\n        plt.savefig(f'{viz_path}/monthly_value_by_unit.png', dpi=300)\n        plt.close()\n\n# 5. Status Distribution by Unit\nif 'STATUS' in df.columns:\n    # Get top 5 statuses\n    top_statuses = df['STATUS'].value_counts().nlargest(5).index.tolist()\n    df_status = df[df['STATUS'].isin(top_statuses)].copy()\n    \n    plt.figure(figsize=(14, 10))\n    status_counts = pd.crosstab(df_status['UNIDADE'], df_status['STATUS'])\n    status_counts.plot(kind='bar', stacked=True, figsize=(14, 10), colormap='tab10')\n    plt.title('Contract Status Distribution by Operating Unit', fontsize=16)\n    plt.xlabel('Operating Unit', fontsize=14)\n    plt.ylabel('Number of Contracts', fontsize=14)\n    plt.legend(title='Status', fontsize=12, title_fontsize=14)\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.savefig(f'{viz_path}/status_distribution_by_unit.png', dpi=300)\n    plt.close()\n\n# 6. Operator Distribution by Unit\nif 'OPERADORA' in df.columns:\n    # Get top operators\n    top_operators = df['OPERADORA'].value_counts().nlargest(5).index.tolist()\n    df_operator = df[df['OPERADORA'].isin(top_operators)].copy()\n    \n    plt.figure(figsize=(14, 10))\n    operator_counts = pd.crosstab(df_operator['UNIDADE'], df_operator['OPERADORA'])\n    operator_counts.plot(kind='bar', stacked=True, figsize=(14, 10), colormap='Set3')\n    plt.title('Insurance Operator Distribution by Operating Unit', fontsize=16)\n    plt.xlabel('Operating Unit', fontsize=14)\n    plt.ylabel('Number of Contracts', fontsize=14)\n    plt.legend(title='Operator', fontsize=12, title_fontsize=14)\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.savefig(f'{viz_path}/operator_distribution_by_unit.png', dpi=300)\n    plt.close()\n\n# 7. Session Count Analysis\nif 'NUM_SESSOES' in df.columns:\n    plt.figure(figsize=(12, 8))\n    session_avg = df.groupby('UNIDADE')['NUM_SESSOES'].mean().sort_values(ascending=False)\n    \n    ax = sns.barplot(x=session_avg.index, y=session_avg.values, palette='Blues_d')\n    plt.title('Average Number of Sessions by Operating Unit', fontsize=16)\n    plt.xlabel('Operating Unit', fontsize=14)\n    plt.ylabel('Average Number of Sessions', fontsize=14)\n    plt.xticks(rotation=45)\n    \n    # Add value labels on top of bars\n    for i, v in enumerate(session_avg):\n        ax.text(i, v + 0.1, f'{v:.1f}', ha='center', fontsize=12)\n    \n    plt.tight_layout()\n    plt.savefig(f'{viz_path}/avg_sessions_by_unit.png', dpi=300)\n    plt.close()\n\n# 8. Payment Status Analysis\nif 'STATUS_FINAL_CONTA' in df.columns:\n    plt.figure(figsize=(14, 10))\n    payment_status = pd.crosstab(df['UNIDADE'], df['STATUS_FINAL_CONTA'])\n    payment_status.plot(kind='bar', stacked=True, figsize=(14, 10), colormap='RdYlGn')\n    plt.title('Payment Status by Operating Unit', fontsize=16)\n    plt.xlabel('Operating Unit', fontsize=14)\n    plt.ylabel('Number of Contracts', fontsize=14)\n    plt.legend(title='Payment Status', fontsize=12, title_fontsize=14)\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.savefig(f'{viz_path}/payment_status_by_unit.png', dpi=300)\n    plt.close()\n\n# 9. Create a comprehensive summary report with key metrics\nsummary_report = {}\nfor unit in df['UNIDADE'].unique():\n    unit_data = df[df['UNIDADE'] == unit]\n    \n    summary_report[unit] = {\n        'contracts': len(unit_data),\n        'total_value': unit_data['VALOR'].sum(),\n        'avg_value': unit_data['VALOR'].mean(),\n        'avg_sessions': unit_data['NUM_SESSOES'].mean() if 'NUM_SESSOES' in unit_data.columns else None,\n        'top_status': unit_data['STATUS'].value_counts().nlargest(1).index[0] if 'STATUS' in unit_data.columns else None,\n        'top_operator': unit_data['OPERADORA'].value_counts().nlargest(1).index[0] if 'OPERADORA' in unit_data.columns else None\n    }\n\n# Convert summary to DataFrame for easier display and save to CSV\nsummary_df = pd.DataFrame.from_dict(summary_report, orient='index')\nsummary_df.to_csv(f'{viz_path}/portfolio_summary.csv')\n\n# Display key performance indicators\nprint(\"=== KEY PORTFOLIO METRICS ===\")\nprint(f\"Total portfolio value: R$ {df['VALOR'].sum():,.2f}\")\nprint(f\"Total number of contracts: {len(df)}\")\nprint(f\"Average contract value: R$ {df['VALOR'].mean():,.2f}\")\nprint(\"\\nUnit Performance Summary:\")\nfor unit, metrics in summary_report.items():\n    print(f\"\\n{unit}:\")\n    print(f\"  - Contracts: {metrics['contracts']:,}\")\n    print(f\"  - Total value: R$ {metrics['total_value']:,.2f}\")\n    print(f\"  - Average value: R$ {metrics['avg_value']:,.2f}\")\n    if metrics['avg_sessions']:\n        print(f\"  - Average sessions: {metrics['avg_sessions']:.1f}\")\n    if metrics['top_status']:\n        print(f\"  - Main status: {metrics['top_status']}\")\n    if metrics['top_operator']:\n        print(f\"  - Main operator: {metrics['top_operator']}\")\n\nprint(f\"\\nVisualization files saved to: {viz_path}\")\nprint(f\"Processed data saved to: {processed_data_path}\")\n# Fix the VALOR conversion issue and create a comprehensive portfolio analysis report\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.ticker import FuncFormatter\nimport re\n\n# Load the original Excel file again\nfile_path = '/data/chats/igk4wd/workspace/uploads/Teste Power BI - Deveras RH.xlsx'\nlocation_sheets = ['Alphaville', 'BH', 'SJC', 'PAULINIA', 'CAMPINAS']\n\n# Create standardized dataframes\nstandardized_dfs = {}\n\n# Define an improved function to convert VALOR to numeric\ndef convert_valor_to_numeric(valor):\n    if pd.isna(valor):\n        return np.nan\n    if isinstance(valor, (int, float)):\n        return float(valor)\n    \n    # Convert string to string format\n    valor_str = str(valor)\n    \n    # Remove any non-numeric characters except . and ,\n    clean_valor = re.sub(r'[^\\d.,]', '', valor_str)\n    \n    # Handle Brazilian/European format (e.g. 1.234,56)\n    if ',' in clean_valor and '.' in clean_valor:\n        # If there are both dots and commas, assume Brazilian/European format\n        clean_valor = clean_valor.replace('.', '').replace(',', '.')\n    elif ',' in clean_valor:\n        # If only commas, replace with dots\n        clean_valor = clean_valor.replace(',', '.')\n    \n    # Convert to float\n    try:\n        return float(clean_valor)\n    except:\n        return np.nan\n\n# Process each sheet and standardize them\nfor sheet in location_sheets:\n    print(f\"Processing {sheet} sheet...\")\n    \n    # Read the sheet\n    df = pd.read_excel(file_path, sheet_name=sheet)\n    \n    # Process VALOR column\n    if 'VALOR' in df.columns:\n        # Save original values for reference\n        df['VALOR_ORIGINAL'] = df['VALOR']\n        \n        # Convert to numeric\n        df['VALOR'] = df['VALOR'].apply(convert_valor_to_numeric)\n        \n        # Check conversion results\n        print(f\"  - Sample original values: {df['VALOR_ORIGINAL'].head(3).tolist()}\")\n        print(f\"  - Sample converted values: {df['VALOR'].head(3).tolist()}\")\n        print(f\"  - NaN values after conversion: {df['VALOR'].isna().sum()}\")\n        print(f\"  - VALOR statistics: Min={df['VALOR'].min()}, Max={df['VALOR'].max()}, Mean={df['VALOR'].mean():.2f}\")\n    \n    # Standardize UNIDADE column\n    if 'UNIDADE' in df.columns:\n        df['UNIDADE'] = df['UNIDADE'].str.strip().str.upper()\n        # Map variations to standard names\n        unit_mapping = {\n            'ALPHA': 'ALPHAVILLE',\n            'alpha ': 'ALPHAVILLE',\n            'SJC': 'SAO JOSE DOS CAMPOS',\n            'BELO HORIZONTE': 'BH',\n            'BSB': 'BRASILIA'\n        }\n        df['UNIDADE'] = df['UNIDADE'].replace(unit_mapping)\n    \n    # Store the standardized dataframe\n    standardized_dfs[sheet] = df\n\n# Combine all sheets into a single dataframe\ncombined_df = pd.concat(standardized_dfs.values(), ignore_index=True)\n\n# Create a directory for visualizations\nimport os\nviz_path = '/data/chats/igk4wd/workspace/visualizations'\nos.makedirs(viz_path, exist_ok=True)\n\n# Set visualization style\nsns.set(style=\"whitegrid\")\nplt.rcParams.update({'font.size': 12})\n\n# Filter extreme values for better visualization (excluding top 1%)\ndf_viz = combined_df.copy()\nif 'VALOR' in df_viz.columns:\n    valor_threshold = df_viz['VALOR'].quantile(0.99)\n    df_viz = df_viz[df_viz['VALOR'] <= valor_threshold]\n\n# 1. Portfolio Value by Operating Unit\nplt.figure(figsize=(12, 8))\nunit_values = df_viz.groupby('UNIDADE')['VALOR'].sum().sort_values(ascending=False)\n\nax = sns.barplot(x=unit_values.index, y=unit_values.values, palette='viridis')\nplt.title('Total Portfolio Value by Operating Unit', fontsize=16)\nplt.xlabel('Operating Unit', fontsize=14)\nplt.ylabel('Total Value (R$)', fontsize=14)\nplt.xticks(rotation=45)\n\n# Add value labels on bars\nfor i, v in enumerate(unit_values):\n    ax.text(i, v + (unit_values.max() * 0.02), f'R$ {v:,.0f}', ha='center', fontsize=11)\n\nplt.tight_layout()\nplt.savefig(f'{viz_path}/total_portfolio_by_unit.png', dpi=300)\nplt.close()\n\n# 2. Average Contract Value by Unit\nplt.figure(figsize=(12, 8))\navg_values = df_viz.groupby('UNIDADE')['VALOR'].mean().sort_values(ascending=False)\n\nax = sns.barplot(x=avg_values.index, y=avg_values.values, palette='rocket')\nplt.title('Average Contract Value by Operating Unit', fontsize=16)\nplt.xlabel('Operating Unit', fontsize=14)\nplt.ylabel('Average Value (R$)', fontsize=14)\nplt.xticks(rotation=45)\n\n# Add value labels on bars\nfor i, v in enumerate(avg_values):\n    ax.text(i, v + (avg_values.max() * 0.02), f'R$ {v:,.0f}', ha='center', fontsize=11)\n\nplt.tight_layout()\nplt.savefig(f'{viz_path}/avg_contract_by_unit.png', dpi=300)\nplt.close()\n\n# 3. Number of Contracts by Unit\nplt.figure(figsize=(12, 8))\ncontract_counts = combined_df.groupby('UNIDADE').size().sort_values(ascending=False)\n\nax = sns.barplot(x=contract_counts.index, y=contract_counts.values, palette='mako')\nplt.title('Number of Contracts by Operating Unit', fontsize=16)\nplt.xlabel('Operating Unit', fontsize=14)\nplt.ylabel('Number of Contracts', fontsize=14)\nplt.xticks(rotation=45)\n\n# Add value labels on bars\nfor i, v in enumerate(contract_counts):\n    ax.text(i, v + (contract_counts.max() * 0.02), f'{v:,}', ha='center', fontsize=11)\n\nplt.tight_layout()\nplt.savefig(f'{viz_path}/contract_count_by_unit.png', dpi=300)\nplt.close()\n\n# 4. Status Distribution by Unit\nif 'STATUS' in combined_df.columns:\n    # Get top 5 statuses\n    top_statuses = combined_df['STATUS'].value_counts().nlargest(5).index.tolist()\n    df_status = combined_df[combined_df['STATUS'].isin(top_statuses)].copy()\n    \n    plt.figure(figsize=(14, 10))\n    status_counts = pd.crosstab(df_status['UNIDADE'], df_status['STATUS'])\n    status_counts.plot(kind='bar', stacked=True, figsize=(14, 10), colormap='tab10')\n    plt.title('Contract Status Distribution by Operating Unit', fontsize=16)\n    plt.xlabel('Operating Unit', fontsize=14)\n    plt.ylabel('Number of Contracts', fontsize=14)\n    plt.legend(title='Status', fontsize=12, title_fontsize=14)\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.savefig(f'{viz_path}/status_distribution_by_unit.png', dpi=300)\n    plt.close()\n\n# 5. Operator Distribution by Unit\nif 'OPERADORA' in combined_df.columns:\n    # Get top operators\n    top_operators = combined_df['OPERADORA'].value_counts().nlargest(5).index.tolist()\n    df_operator = combined_df[combined_df['OPERADORA'].isin(top_operators)].copy()\n    \n    plt.figure(figsize=(14, 10))\n    operator_counts = pd.crosstab(df_operator['UNIDADE'], df_operator['OPERADORA'])\n    operator_counts.plot(kind='bar', stacked=True, figsize=(14, 10), colormap='Set3')\n    plt.title('Insurance Operator Distribution by Operating Unit', fontsize=16)\n    plt.xlabel('Operating Unit', fontsize=14)\n    plt.ylabel('Number of Contracts', fontsize=14)\n    plt.legend(title='Operator', fontsize=12, title_fontsize=14)\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.savefig(f'{viz_path}/operator_distribution_by_unit.png', dpi=300)\n    plt.close()\n\n# 6. Sessions Distribution by Unit\nif 'Nº SESSÕES' in combined_df.columns:\n    plt.figure(figsize=(12, 8))\n    session_avg = combined_df.groupby('UNIDADE')['Nº SESSÕES'].mean().sort_values(ascending=False)\n    \n    ax = sns.barplot(x=session_avg.index, y=session_avg.values, palette='Blues_d')\n    plt.title('Average Number of Sessions by Operating Unit', fontsize=16)\n    plt.xlabel('Operating Unit', fontsize=14)\n    plt.ylabel('Average Number of Sessions', fontsize=14)\n    plt.xticks(rotation=45)\n    \n    # Add value labels on bars\n    for i, v in enumerate(session_avg):\n        ax.text(i, v + 0.1, f'{v:.1f}', ha='center', fontsize=11)\n    \n    plt.tight_layout()\n    plt.savefig(f'{viz_path}/avg_sessions_by_unit.png', dpi=300)\n    plt.close()\n\n# 7. Boxplot of VALOR by Unit\nplt.figure(figsize=(14, 10))\nsns.boxplot(x='UNIDADE', y='VALOR', data=df_viz, palette='Set2')\nplt.title('Distribution of Contract Values by Unit', fontsize=16)\nplt.xlabel('Operating Unit', fontsize=14)\nplt.ylabel('Contract Value (R$)', fontsize=14)\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.savefig(f'{viz_path}/valor_distribution_by_unit.png', dpi=300)\nplt.close()\n\n# Create the comprehensive summary report\nsummary_data = {}\n\nfor unit in combined_df['UNIDADE'].unique():\n    unit_data = combined_df[combined_df['UNIDADE'] == unit]\n    \n    # Calculate key metrics\n    summary_data[unit] = {\n        'n_contracts': len(unit_data),\n        'total_value': unit_data['VALOR'].sum(),\n        'avg_value': unit_data['VALOR'].mean(),\n        'max_value': unit_data['VALOR'].max(),\n        'min_value': unit_data['VALOR'].min()\n    }\n    \n    # Add sessions data if available\n    if 'Nº SESSÕES' in unit_data.columns:\n        summary_data[unit]['avg_sessions'] = unit_data['Nº SESSÕES'].mean()\n    \n    # Add status distribution if available\n    if 'STATUS' in unit_data.columns and not unit_data['STATUS'].isna().all():\n        top_status = unit_data['STATUS'].value_counts().index[0] if not unit_data['STATUS'].value_counts().empty else 'N/A'\n        summary_data[unit]['top_status'] = top_status\n    \n    # Add operator distribution if available\n    if 'OPERADORA' in unit_data.columns and not unit_data['OPERADORA'].isna().all():\n        top_operator = unit_data['OPERADORA'].value_counts().index[0] if not unit_data['OPERADORA'].value_counts().empty else 'N/A'\n        summary_data[unit]['top_operator'] = top_operator\n\n# Convert to DataFrame and save\nsummary_df = pd.DataFrame.from_dict(summary_data, orient='index')\nsummary_df.to_csv(f'{viz_path}/portfolio_summary.csv')\n\n# Display key portfolio metrics\nprint(\"\\n=== PORTFOLIO PERFORMANCE ANALYSIS ===\")\nprint(f\"Total portfolio value: R$ {combined_df['VALOR'].sum():,.2f}\")\nprint(f\"Total number of contracts: {len(combined_df):,}\")\nprint(f\"Average contract value: R$ {combined_df['VALOR'].mean():,.2f}\")\n\n# Display unit performance summary\nprint(\"\\n=== UNIT PERFORMANCE SUMMARY ===\")\nfor unit, metrics in summary_data.items():\n    print(f\"\\n{unit}:\")\n    print(f\"  - Contracts: {metrics['n_contracts']:,}\")\n    print(f\"  - Total value: R$ {metrics['total_value']:,.2f}\")\n    print(f\"  - Average value: R$ {metrics['avg_value']:,.2f}\")\n    \n    if 'avg_sessions' in metrics:\n        print(f\"  - Average sessions: {metrics['avg_sessions']:.1f}\")\n    \n    if 'top_status' in metrics:\n        print(f\"  - Main status: {metrics['top_status']}\")\n    \n    if 'top_operator' in metrics:\n        print(f\"  - Main operator: {metrics['top_operator']}\")\n\n# Save the processed data\nprocessed_data_path = '/data/chats/igk4wd/workspace/processed_data.csv'\ncombined_df.to_csv(processed_data_path, index=False)\nprint(f\"\\nProcessed data saved to: {processed_data_path}\")\nprint(f\"Visualization files saved to: {viz_path}\")\n\n# List the identified data inconsistencies\nprint(\"\\n=== DATA INCONSISTENCIES ===\")\nprint(\"1. Inconsistent column naming across sheets\")\nprint(\"2. Inconsistent UNIDADE names (e.g., 'ALPHAVILLE', 'alpha ')\")\nprint(\"3. Multiple patient names assigned to the same client ID\")\nprint(\"4. Inconsistent data types for common columns across sheets\")\nprint(\"5. Unusual date values (dates before 2000 likely errors)\")\nprint(\"6. Extreme values in VALOR column that required filtering\")\nprint(\"7. Missing values in critical columns\")\n# Fix the extreme VALOR values in SJC and PAULINIA sheets \n# and create a final comprehensive portfolio analysis report\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.ticker import FuncFormatter\nimport re\nimport os\nfrom datetime import datetime\n\n# Load the original Excel file again to fix the data issues\nfile_path = '/data/chats/igk4wd/workspace/uploads/Teste Power BI - Deveras RH.xlsx'\nlocation_sheets = ['Alphaville', 'BH', 'SJC', 'PAULINIA', 'CAMPINAS']\n\n# Create standardized dataframes\nstandardized_dfs = {}\n\n# Improved function to convert VALOR to numeric, handling special cases\ndef convert_valor_to_numeric(valor):\n    if pd.isna(valor):\n        return np.nan\n    \n    # If it's already a number, but actually a date\n    if isinstance(valor, (int, float)) and valor > 1000000:  # Very large numbers are likely date formats\n        return np.nan\n    \n    # If it's already a reasonable number\n    if isinstance(valor, (int, float)) and valor < 1000000:\n        return float(valor)\n    \n    # If it's a datetime object (common issue in some sheets)\n    if isinstance(valor, datetime):\n        return np.nan\n    \n    # Convert string to string format for processing\n    valor_str = str(valor)\n    \n    # Check if it might be a date in disguise\n    if len(valor_str) > 10 and re.match(r'\\d{10,}', valor_str):\n        return np.nan\n    \n    # Remove any non-numeric characters except . and ,\n    clean_valor = re.sub(r'[^\\d.,]', '', valor_str)\n    \n    # Handle Brazilian/European format (e.g. 1.234,56)\n    if ',' in clean_valor and '.' in clean_valor:\n        clean_valor = clean_valor.replace('.', '').replace(',', '.')\n    elif ',' in clean_valor:\n        clean_valor = clean_valor.replace(',', '.')\n    \n    # Convert to float\n    try:\n        val = float(clean_valor)\n        # Additional sanity check - extreme values are likely errors\n        if val > 500000:  # Assuming no contract should be above 500,000\n            return np.nan\n        return val\n    except:\n        return np.nan\n\n# Process each sheet and standardize them\nprint(\"=== DATA CLEANING AND STANDARDIZATION ===\")\nfor sheet in location_sheets:\n    print(f\"\\nProcessing {sheet} sheet...\")\n    \n    # Read the sheet\n    df = pd.read_excel(file_path, sheet_name=sheet)\n    \n    # Process VALOR column\n    if 'VALOR' in df.columns:\n        # Save original values for reference\n        df['VALOR_ORIGINAL'] = df['VALOR']\n        \n        # Convert to numeric with improved function\n        df['VALOR'] = df['VALOR'].apply(convert_valor_to_numeric)\n        \n        # Check conversion results\n        print(f\"  - Original values (sample): {df['VALOR_ORIGINAL'].head(3).tolist()}\")\n        print(f\"  - Converted values (sample): {df['VALOR'].head(3).tolist()}\")\n        print(f\"  - NaN values after conversion: {df['VALOR'].isna().sum()} out of {len(df)}\")\n        print(f\"  - VALOR statistics: Min={df['VALOR'].min():.2f}, Max={df['VALOR'].max():.2f}, Mean={df['VALOR'].mean():.2f}\")\n    \n    # Clean and standardize column names\n    df.columns = [col.strip() for col in df.columns]\n    \n    # Standardize UNIDADE column\n    if 'UNIDADE' in df.columns:\n        df['UNIDADE'] = df['UNIDADE'].str.strip().str.upper()\n        # Map variations to standard names\n        unit_mapping = {\n            'ALPHA': 'ALPHAVILLE',\n            'alpha ': 'ALPHAVILLE',\n            'SJC': 'SAO JOSE DOS CAMPOS',\n            'BELO HORIZONTE': 'BH',\n            'BSB': 'BRASILIA'\n        }\n        df['UNIDADE'] = df['UNIDADE'].replace(unit_mapping)\n    \n    # Add source sheet information\n    df['FONTE_PLANILHA'] = sheet\n    \n    # Store the cleaned dataframe\n    standardized_dfs[sheet] = df\n\n# Combine all sheets into a single dataframe\ncombined_df = pd.concat(standardized_dfs.values(), ignore_index=True)\n\n# Further cleaning: Remove rows with NaN VALOR and filter extreme values\nvalid_valor_df = combined_df[combined_df['VALOR'].notna()].copy()\nprint(f\"\\nRows with valid VALOR: {len(valid_valor_df)} out of {len(combined_df)} total rows\")\n\n# Create directory for visualizations and reports\nviz_path = '/data/chats/igk4wd/workspace/visualizations'\nos.makedirs(viz_path, exist_ok=True)\n\n# Set visualization style\nsns.set(style=\"whitegrid\")\nplt.rcParams.update({'font.size': 12})\n\n# ====== PORTFOLIO ANALYSIS VISUALIZATIONS ======\n\n# 1. Portfolio Value by Operating Unit\nplt.figure(figsize=(12, 8))\n# Group by UNIDADE and sum the VALOR, handling NaN values\nunit_values = valid_valor_df.groupby('UNIDADE')['VALOR'].sum().sort_values(ascending=False)\n\nax = sns.barplot(x=unit_values.index, y=unit_values.values, palette='viridis')\nplt.title('Total Portfolio Value by Operating Unit', fontsize=16)\nplt.xlabel('Operating Unit', fontsize=14)\nplt.ylabel('Total Value (R$)', fontsize=14)\nplt.xticks(rotation=45)\n\n# Add value labels on bars\nfor i, v in enumerate(unit_values):\n    ax.text(i, v + (unit_values.max() * 0.02), f'R$ {v:,.0f}', ha='center', fontsize=11)\n\nplt.tight_layout()\nplt.savefig(f'{viz_path}/total_portfolio_by_unit.png', dpi=300)\nplt.close()\n\n# 2. Average Contract Value by Unit\nplt.figure(figsize=(12, 8))\navg_values = valid_valor_df.groupby('UNIDADE')['VALOR'].mean().sort_values(ascending=False)\n\nax = sns.barplot(x=avg_values.index, y=avg_values.values, palette='rocket')\nplt.title('Average Contract Value by Operating Unit', fontsize=16)\nplt.xlabel('Operating Unit', fontsize=14)\nplt.ylabel('Average Value (R$)', fontsize=14)\nplt.xticks(rotation=45)\n\n# Add value labels on bars\nfor i, v in enumerate(avg_values):\n    ax.text(i, v + (avg_values.max() * 0.02), f'R$ {v:,.0f}', ha='center', fontsize=11)\n\nplt.tight_layout()\nplt.savefig(f'{viz_path}/avg_contract_by_unit.png', dpi=300)\nplt.close()\n\n# 3. Number of Contracts by Unit\nplt.figure(figsize=(12, 8))\ncontract_counts = combined_df.groupby('UNIDADE').size().sort_values(ascending=False)\n\nax = sns.barplot(x=contract_counts.index, y=contract_counts.values, palette='mako')\nplt.title('Number of Contracts by Operating Unit', fontsize=16)\nplt.xlabel('Operating Unit', fontsize=14)\nplt.ylabel('Number of Contracts', fontsize=14)\nplt.xticks(rotation=45)\n\n# Add value labels on bars\nfor i, v in enumerate(contract_counts):\n    ax.text(i, v + (contract_counts.max() * 0.02), f'{v:,}', ha='center', fontsize=11)\n\nplt.tight_layout()\nplt.savefig(f'{viz_path}/contract_count_by_unit.png', dpi=300)\nplt.close()\n\n# 4. Status Distribution by Unit (Top 5)\nif 'STATUS' in combined_df.columns:\n    # Get top 5 statuses\n    top_statuses = combined_df['STATUS'].value_counts().nlargest(5).index.tolist()\n    df_status = combined_df[combined_df['STATUS'].isin(top_statuses)].copy()\n    \n    plt.figure(figsize=(14, 10))\n    status_counts = pd.crosstab(df_status['UNIDADE'], df_status['STATUS'])\n    status_counts.plot(kind='bar', stacked=True, figsize=(14, 10), colormap='tab10')\n    plt.title('Contract Status Distribution by Operating Unit', fontsize=16)\n    plt.xlabel('Operating Unit', fontsize=14)\n    plt.ylabel('Number of Contracts', fontsize=14)\n    plt.legend(title='Status', fontsize=12, title_fontsize=14)\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.savefig(f'{viz_path}/status_distribution_by_unit.png', dpi=300)\n    plt.close()\n\n# 5. Operator Distribution by Unit (Top 5)\nif 'OPERADORA' in combined_df.columns:\n    # Get top operators\n    top_operators = combined_df['OPERADORA'].value_counts().nlargest(5).index.tolist()\n    df_operator = combined_df[combined_df['OPERADORA'].isin(top_operators)].copy()\n    \n    plt.figure(figsize=(14, 10))\n    operator_counts = pd.crosstab(df_operator['UNIDADE'], df_operator['OPERADORA'])\n    operator_counts.plot(kind='bar', stacked=True, figsize=(14, 10), colormap='Set3')\n    plt.title('Insurance Operator Distribution by Operating Unit', fontsize=16)\n    plt.xlabel('Operating Unit', fontsize=14)\n    plt.ylabel('Number of Contracts', fontsize=14)\n    plt.legend(title='Operator', fontsize=12, title_fontsize=14)\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.savefig(f'{viz_path}/operator_distribution_by_unit.png', dpi=300)\n    plt.close()\n\n# 6. Sessions Distribution by Unit\nif 'Nº SESSÕES' in combined_df.columns:\n    plt.figure(figsize=(12, 8))\n    session_avg = combined_df.groupby('UNIDADE')['Nº SESSÕES'].mean().sort_values(ascending=False)\n    \n    ax = sns.barplot(x=session_avg.index, y=session_avg.values, palette='Blues_d')\n    plt.title('Average Number of Sessions by Operating Unit', fontsize=16)\n    plt.xlabel('Operating Unit', fontsize=14)\n    plt.ylabel('Average Number of Sessions', fontsize=14)\n    plt.xticks(rotation=45)\n    \n    # Add value labels on bars\n    for i, v in enumerate(session_avg):\n        ax.text(i, v + 0.1, f'{v:.1f}', ha='center', fontsize=11)\n    \n    plt.tight_layout()\n    plt.savefig(f'{viz_path}/avg_sessions_by_unit.png', dpi=300)\n    plt.close()\n\n# 7. Boxplot of VALOR by Unit\nplt.figure(figsize=(14, 10))\nsns.boxplot(x='UNIDADE', y='VALOR', data=valid_valor_df, palette='Set2')\nplt.title('Distribution of Contract Values by Unit', fontsize=16)\nplt.xlabel('Operating Unit', fontsize=14)\nplt.ylabel('Contract Value (R$)', fontsize=14)\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.savefig(f'{viz_path}/valor_distribution_by_unit.png', dpi=300)\nplt.close()\n\n# Create a comprehensive summary report for each operating unit\nsummary_data = {}\n\nfor unit in combined_df['UNIDADE'].unique():\n    unit_data = combined_df[combined_df['UNIDADE'] == unit]\n    unit_valid_valor = unit_data[unit_data['VALOR'].notna()]\n    \n    # Calculate key metrics\n    summary_data[unit] = {\n        'n_contracts': len(unit_data),\n        'n_valid_contracts': len(unit_valid_valor),\n        'total_value': unit_valid_valor['VALOR'].sum(),\n        'avg_value': unit_valid_valor['VALOR'].mean(),\n        'max_value': unit_valid_valor['VALOR'].max(),\n        'min_value': unit_valid_valor['VALOR'].min(),\n        'data_completeness': len(unit_valid_valor) / len(unit_data) if len(unit_data) > 0 else 0,\n    }\n    \n    # Add sessions data if available\n    if 'Nº SESSÕES' in unit_data.columns:\n        summary_data[unit]['avg_sessions'] = unit_data['Nº SESSÕES'].mean()\n    \n    # Add status distribution if available\n    if 'STATUS' in unit_data.columns and not unit_data['STATUS'].isna().all():\n        status_counts = unit_data['STATUS'].value_counts()\n        top_status = status_counts.index[0] if not status_counts.empty else 'N/A'\n        top_status_pct = status_counts.iloc[0] / len(unit_data) if not status_counts.empty else 0\n        summary_data[unit]['top_status'] = top_status\n        summary_data[unit]['top_status_pct'] = top_status_pct\n    \n    # Add operator distribution if available\n    if 'OPERADORA' in unit_data.columns and not unit_data['OPERADORA'].isna().all():\n        operator_counts = unit_data['OPERADORA'].value_counts()\n        top_operator = operator_counts.index[0] if not operator_counts.empty else 'N/A'\n        top_operator_pct = operator_counts.iloc[0] / len(unit_data) if not operator_counts.empty else 0\n        summary_data[unit]['top_operator'] = top_operator\n        summary_data[unit]['top_operator_pct'] = top_operator_pct\n\n# Convert to DataFrame for reporting\nsummary_df = pd.DataFrame.from_dict(summary_data, orient='index')\nsummary_df.to_csv(f'{viz_path}/portfolio_summary.csv')\n\n# Create a comprehensive final report\nreport_path = f'{viz_path}/portfolio_analysis_report.md'\nwith open(report_path, 'w') as f:\n    f.write(\"# Portfolio Analysis Report - Deveras RH\\n\\n\")\n    \n    f.write(\"## Data Overview\\n\")\n    f.write(f\"- Total contracts analyzed: {len(combined_df):,}\\n\")\n    f.write(f\"- Contracts with valid financial data: {len(valid_valor_df):,} ({len(valid_valor_df)/len(combined_df):.1%})\\n\")\n    f.write(f\"- Operating units: {', '.join(combined_df['UNIDADE'].unique())}\\n\")\n    f.write(f\"- Total portfolio value: R$ {valid_valor_df['VALOR'].sum():,.2f}\\n\\n\")\n    \n    f.write(\"## Key Portfolio Metrics by Operating Unit\\n\\n\")\n    \n    for unit, metrics in summary_data.items():\n        f.write(f\"### {unit}\\n\")\n        f.write(f\"- Contracts: {metrics['n_contracts']:,}\\n\")\n        f.write(f\"- Contracts with valid financial data: {metrics['n_valid_contracts']:,} ({metrics['data_completeness']:.1%})\\n\")\n        f.write(f\"- Total value: R$ {metrics['total_value']:,.2f}\\n\")\n        f.write(f\"- Average value per contract: R$ {metrics['avg_value']:,.2f}\\n\")\n        f.write(f\"- Value range: R$ {metrics['min_value']:,.2f} to R$ {metrics['max_value']:,.2f}\\n\")\n        \n        if 'avg_sessions' in metrics:\n            f.write(f\"- Average sessions: {metrics['avg_sessions']:.1f}\\n\")\n        \n        if 'top_status' in metrics:\n            f.write(f\"- Primary status: {metrics['top_status']} ({metrics['top_status_pct']:.1%} of contracts)\\n\")\n        \n        if 'top_operator' in metrics:\n            f.write(f\"- Primary operator: {metrics['top_operator']} ({metrics['top_operator_pct']:.1%} of contracts)\\n\")\n        \n        f.write(\"\\n\")\n    \n    f.write(\"## Data Inconsistencies Identified\\n\\n\")\n    f.write(\"1. **Inconsistent Column Naming**: Different sheets used varied naming conventions for the same data.\\n\")\n    f.write(\"2. **Inconsistent Location Names**: The same operating unit appeared under different names (e.g., 'ALPHAVILLE', 'alpha').\\n\")\n    f.write(\"3. **Client ID Inconsistencies**: Multiple patient names were assigned to the same client ID.\\n\")\n    f.write(\"4. **Data Type Inconsistencies**: The same columns had different data types across sheets.\\n\")\n    f.write(\"5. **Problematic Date Values**: Some sheets contained unusual date values (dates before 2000).\\n\")\n    f.write(\"6. **Extreme Values in VALOR Column**: SJC and PAULINIA sheets contained datetime values incorrectly interpreted as currency.\\n\")\n    f.write(\"7. **Missing Values**: Critical columns contained significant numbers of missing values.\\n\\n\")\n    \n    f.write(\"## Recommendations for Dashboard Creation\\n\\n\")\n    f.write(\"1. Use the standardized processed data provided in the CSV file for consistent analysis.\\n\")\n    f.write(\"2. Implement filters by operating unit, status, and insurance operator.\\n\")\n    f.write(\"3. Include time series visualizations to track portfolio growth over time.\\n\")\n    f.write(\"4. Create separate panels for individual operating unit performance and consolidated views.\\n\")\n    f.write(\"5. Consider implementing alerts for contracts with unusual values or statuses.\\n\")\n    f.write(\"6. Include data quality metrics to monitor completeness and consistency.\\n\")\n\n# Save the processed data with fixed values\nprocessed_data_path = '/data/chats/igk4wd/workspace/processed_data.csv'\ncombined_df.to_csv(processed_data_path, index=False)\n\n# Display summary of the analysis\nprint(\"\\n=== PORTFOLIO PERFORMANCE ANALYSIS ===\")\nprint(f\"Total portfolio value: R$ {valid_valor_df['VALOR'].sum():,.2f}\")\nprint(f\"Total number of contracts: {len(combined_df):,}\")\nprint(f\"Contracts with valid financial data: {len(valid_valor_df):,} ({len(valid_valor_df)/len(combined_df):.1%})\")\nprint(f\"Average contract value: R$ {valid_valor_df['VALOR'].mean():,.2f}\")\n\n# Display unit performance summary\nprint(\"\\n=== UNIT PERFORMANCE SUMMARY ===\")\nfor unit, metrics in summary_data.items():\n    print(f\"\\n{unit}:\")\n    print(f\"  - Contracts: {metrics['n_contracts']:,}\")\n    print(f\"  - Valid financial data: {metrics['n_valid_contracts']:,} ({metrics['data_completeness']:.1%})\")\n    print(f\"  - Total value: R$ {metrics['total_value']:,.2f}\")\n    print(f\"  - Average value: R$ {metrics['avg_value']:,.2f}\")\n    \n    if 'avg_sessions' in metrics:\n        print(f\"  - Average sessions: {metrics['avg_sessions']:.1f}\")\n    \n    if 'top_status' in metrics:\n        print(f\"  - Main status: {metrics['top_status']}\")\n    \n    if 'top_operator' in metrics:\n        print(f\"  - Main operator: {metrics['top_operator']}\")\n\nprint(f\"\\nDetailed report saved to: {report_path}\")\nprint(f\"Processed data saved to: {processed_data_path}\")\nprint(f\"Visualization files saved to: {viz_path}\")",
        "result": "\nExcel sheets: ['BASE DE DADOS', 'Orientações', 'Alphaville', 'BH', 'SJC', 'PAULINIA', 'CAMPINAS']\n\n--- Sheet: BASE DE DADOS ---\nShape: (16, 2)\nColumn names: ['CÓDIGO DO CLIENTE:', 'Unnamed: 1']\nData types:\n CÓDIGO DO CLIENTE:     object\nUnnamed: 1            float64\ndtype: object\nFirst 3 rows:\n           CÓDIGO DO CLIENTE:  Unnamed: 1\n0  NOME COMPLETO DO CLIENTE:         NaN\n1                       CPF:         NaN\n2                  ENDEREÇO:         NaN\nMissing values:\n Unnamed: 1    12\ndtype: int64\nNumber of duplicate rows: 0\n\n--- Sheet: Orientações ---\nShape: (17, 2)\nColumn names: ['Unnamed: 0', 'Unnamed: 1']\nData types:\n Unnamed: 0    object\nUnnamed: 1    object\ndtype: object\nFirst 3 rows:\n   Unnamed: 0 Unnamed: 1\n0        NaN        NaN\n1        NaN        NaN\n2        NaN        NaN\nMissing values:\n Unnamed: 0     9\nUnnamed: 1    11\ndtype: int64\nNumber of duplicate rows: 8\n\n--- Sheet: Alphaville ---\nShape: (1651, 45)\nColumn names: ['Nº CONTRATO (SISTEMA GERA)', 'STATUS', 'CÓD CLIENTE', 'PACIENTYES', 'UNIDADE', 'OPERADORA', ' NF', 'DATA EMISSÃO NF', 'COMPETÊNCIA', 'QUINZENA', 'VALOR', 'Nº SESSÕES', 'STATUS PRÉVIO DA CONTA', 'SOLICITANTE', 'STATUS ANÁLISE DE CRÉDITO', 'STATUS JURÍDICO CLIENTE', '1ª APROVAÇÃO CLÍNICA GARANTIDORA', '2ª APROVAÇÃO FACTORING', 'CONTRATO ASSINADO', 'DATA DO CONTRATO', 'FINANCIADO', 'VALOR FINANCIADO', 'DATA FINANCIAMENTO', 'PAGAMENTO CLINICA', 'VALOR PAGAMENTO CLÍNICA', 'DATA PAGAMENTO CLÍNICA', 'TRANSMISSÃO DA CONTA', 'DATA PROTOCOLO ', 'DATA DE VENCIMENTO', 'STATUS CONTAS A RECEBER', 'VALOR PAGO', 'DATA DO PAGAMENTO REEMBOLSO', 'VALOR GLOSADO', 'DATA DO PAGAMENTO FACTORING', 'VALOR PAGAMENTO FACTORING', 'CONTAS A RECEBER', 'STATUS FINAL DA CONTA', 'LIGAÇÃO ', 'DATA DA LIGAÇÃO', 'RECLAMAÇÃO CHAT', 'DATA DA RECLAMAÇÃO', 'NIP', 'DATA DA NIP', 'ETAPAS DA JUDICILIAZAÇÃO', 'OBSERVAÇÕES']\nData types:\n Nº CONTRATO (SISTEMA GERA)                   int64\nSTATUS                                      object\nCÓD CLIENTE                                  int64\nPACIENTYES                                  object\nUNIDADE                                     object\nOPERADORA                                   object\n NF                                        float64\nDATA EMISSÃO NF                     datetime64[ns]\nCOMPETÊNCIA                         datetime64[ns]\nQUINZENA                                   float64\nVALOR                                       object\nNº SESSÕES                                   int64\nSTATUS PRÉVIO DA CONTA                      object\nSOLICITANTE                                 object\nSTATUS ANÁLISE DE CRÉDITO                   object\nSTATUS JURÍDICO CLIENTE                     object\n1ª APROVAÇÃO CLÍNICA GARANTIDORA            object\n2ª APROVAÇÃO FACTORING                      object\nCONTRATO ASSINADO                           object\nDATA DO CONTRATO                            object\nFINANCIADO                                  object\nVALOR FINANCIADO                           float64\nDATA FINANCIAMENTO                          object\nPAGAMENTO CLINICA                           object\nVALOR PAGAMENTO CLÍNICA                     object\nDATA PAGAMENTO CLÍNICA              datetime64[ns]\nTRANSMISSÃO DA CONTA                        object\nDATA PROTOCOLO                      datetime64[ns]\nDATA DE VENCIMENTO                  datetime64[ns]\nSTATUS CONTAS A RECEBER                     object\nVALOR PAGO                                  object\nDATA DO PAGAMENTO REEMBOLSO                 object\nVALOR GLOSADO                               object\nDATA DO PAGAMENTO FACTORING         datetime64[ns]\nVALOR PAGAMENTO FACTORING                  float64\nCONTAS A RECEBER                            object\nSTATUS FINAL DA CONTA                       object\nLIGAÇÃO                                     object\nDATA DA LIGAÇÃO                     datetime64[ns]\nRECLAMAÇÃO CHAT                             object\nDATA DA RECLAMAÇÃO                  datetime64[ns]\nNIP                                         object\nDATA DA NIP                         datetime64[ns]\nETAPAS DA JUDICILIAZAÇÃO                    object\nOBSERVAÇÕES                                 object\ndtype: object\nFirst 3 rows:\n    Nº CONTRATO (SISTEMA GERA)                   STATUS  CÓD CLIENTE  \\\n0                           1                    ATIVO           10   \n1                           2  INATIVO - TRANSPLANTADO           20   \n2                           3     INATIVO - TRANSFERID           30   \n\n    PACIENTYES     UNIDADE OPERADORA      NF DATA EMISSÃO NF COMPETÊNCIA  \\\n0  PEDRO SILVA  ALPHAVILLE  BRADESCO  3270.0      2023-01-30  2023-03-01   \n1  PEDRO SILVA      alpha   BRADESCO  3271.0      2023-03-02  2023-04-01   \n2  PEDRO SILVA  ALPHAVILLE  BRADESCO  3302.0      2023-03-02  2023-04-01   \n\n   QUINZENA  ... CONTAS A RECEBER  STATUS FINAL DA CONTA LIGAÇÃO   \\\n0       2.0  ...                0                REGULAR      SIM   \n1       1.0  ...                0         GLOSA INDEVIDA      NÃO   \n2       2.0  ...  \n--- Detailed analysis of Alphaville sheet ---\nVALOR column sample values: [41545.78, 54270.99, 43074.45]\nConverted VALOR sample: [nan, nan, nan]\nNaN values after conversion: 1651\nColumn count: 46\n\nNumeric columns statistics:\n                             count          mean           std      min  \\\nNº CONTRATO (SISTEMA GERA)  1651.0    826.000000    476.746963      1.0   \nCÓD CLIENTE                 1651.0   8260.000000   4767.469629     10.0   \n NF                         1646.0   4019.770960    596.754145   2965.0   \nQUINZENA                    1647.0      1.485124      0.499930      1.0   \nNº SESSÕES                  1651.0     14.933374      3.530188      9.0   \nVALOR FINANCIADO            1651.0  47518.230999  21057.175113   3000.0   \nVALOR PAGAMENTO FACTORING      9.0  37654.580000  14857.632211  10000.0   \nVALOR_NUMERIC                  0.0           NaN           NaN      NaN   \n\n                                 25%       50%        75%        max  \nNº CONTRATO (SISTEMA GERA)    413.50    826.00   1238.500    1651.00  \nCÓD CLIENTE                  4135.00   8260.00  12385.000   16510.00  \n NF                          3511.50   4023.50   4538.750    5034.00  \nQUINZENA                        1.00      1.00      2.000       2.00  \nNº SESSÕES                     13.00     15.00     19.000      20.00  \nVALOR FINANCIADO            30608.21  50228.69  63148.495  124526.12  \nVALOR PAGAMENTO FACTORING   30000.00  41545.78  50000.000   54270.99  \nVALOR_NUMERIC                    NaN       NaN        NaN        NaN  \n\nCategorical columns unique values count:\nSTATUS: 9 unique values\nTop 3 values: STATUS\nATIVO                      710\nINATIVO - TRANSPLANTADO    118\nINATIVO - TRANSFERID       118\nName: count, dtype: int64\nPACIENTYES: 33 unique values\nTop 3 values: PACIENTYES\nGabriel Souza Lima         188\nDaniel Teixeira Costa      166\nLeonardo Mendes Almeida    128\nName: count, dtype: int64\nUNIDADE: 2 unique values\nTop 3 values: UNIDADE\nALPHAVILLE    1633\nalpha           18\nName: count, dtype: int64\nOPERADORA: 8 unique values\nTop 3 values: OPERADORA\nBRADESCO          695\nSUL AMERICA       502\nSEGUROS UNIMED    129\nName: count, dtype: int64\nVALOR: 1492 unique values\nTop 3 values: VALOR\n19500    23\n16900    22\n15600    19\nName: count, dtype: int64\n\nDate ranges:\nDATA EMISSÃO NF: 1899-11-30 00:00:00 to 2024-07-02 00:00:00\nCOMPETÊNCIA: 2023-01-01 00:00:00 to 2024-08-01 00:00:00\nDATA PAGAMENTO CLÍNICA: 2023-01-06 00:00:00 to 2024-08-29 00:00:00\nDATA PROTOCOLO : 2023-01-15 00:00:00 to 2024-08-29 00:00:00\nDATA DE VENCIMENTO: 1900-01-30 00:00:00 to 2024-09-29 00:00:00\nDATA DO PAGAMENTO FACTORING: 2023-06-26 00:00:00 to 2024-06-01 00:00:00\nDATA DA LIGAÇÃO: 1900-02-28 00:00:00 to 2024-07-31 00:00:00\nDATA DA RECLAMAÇÃO: 1900-03-30 00:00:00 to 2024-08-30 00:00:00\nDATA DA NIP: 1900-04-25 00:00:00 to 2024-09-25 00:00:00\n\n--- Detailed analysis of BH sheet ---\nVALOR column sample values: [51231.24, 47328.33, 57888.45]\nConverted VALOR sample: [nan, nan, nan]\nNaN values after conversion: 814\nColumn count: 46\n\nNumeric columns statistics:\n                             count          mean           std      min  \\\nNº CONTRATO (SISTEMA GERA)2  814.0    401.980344    202.705094     25.0   \nCÓD CLIENTE                  814.0    588.352580    314.332033     25.0   \nDATA EMISSÃO NF              814.0  44985.426290   2253.224774    -20.0   \nCOMPETÊNCIA                  812.0  45116.277094    286.705472  44197.0   \nQUINZENA                     810.0      1.492593      0.500254      1.0   \nNº SESSÕES                   814.0     12.566339      1.593312     10.0   \nDATA FINANCIAMENTO           814.0  45281.986486    193.137820  44927.0   \nDATA PROTOCOLO               765.0  45282.162092    175.922683  44944.0   \nDATA DE VENCIMENTO           812.0  33436.859606  19939.932835     20.0   \nCONTAS A RECEBER             804.0  20261.605224  17342.851594 -41740.0   \nDATA DA LIGAÇÃO              814.0  42716.482801  10329.519296     56.0   \nDATA DA RECLAMAÇÃO           814.0  42776.733415  10329.580108    116.0   \nDATA DA NIP                  814.0  42792.750614  10329.584463    132.0   \nVALOR_NUMERIC                  0.0           NaN           NaN      NaN   \n\n                                   25%       50%       75%       max  \nNº CONTRATO (SISTEMA GERA)2    228.250    402.50    576.75    780.00  \nCÓD CLIENTE                    317.000    588.00    859.25   1176.00  \nDATA EMISSÃO NF              44891.000  45142.00  45347.00  45500.00  \nCOMPETÊNCIA                  44911.000  45170.00  45367.00  45520.00  \nQUINZENA                         1.000      1.00      2.00      2.00  \nNº SESSÕES                      11.000     13.00     14.00     15.00  \nDATA FINANCIAMENTO           45108.000  45346.00  45467.00  45528.00  \nDATA PROTOCOLO               45125.000  45299.00  45448.00  45533.00  \nDATA DE VENCIMENTO              20.000  45202.50  45429.50  45655.00  \nCONTAS A RECEBER              6192.605  19498.22  35535.45  84354.34  \nDATA DA LIGAÇÃO         \n--- Processing Alphaville sheet ---\nConverted VALOR sample: [41545.78, 54270.99, 43074.45]\nNaN values after conversion: 21\nVALOR statistics: Min=3000.0, Max=124526.12, Mean=47585.67782822086\nUNIDADE values in Alphaville: ['ALPHAVILLE' 'alpha ']\n\n--- Processing BH sheet ---\nConverted VALOR sample: [51231.24, 47328.33, 57888.45]\nNaN values after conversion: 52\nVALOR statistics: Min=8.0, Max=84354.34, Mean=28020.540530971128\nUNIDADE values in BH: ['BELO HORIZONTE' 'OSASCO' 'ALPHAVILLE' 'BSB']\n\n--- Processing SJC sheet ---\nConverted VALOR sample: [20140120092136.0, 20160529185736.0, 20160829092136.0]\nNaN values after conversion: 18\nVALOR statistics: Min=2600.0, Max=26030602074048.0, Mean=11384180980684.389\nUNIDADE values in SJC: ['DOMICILIAR' 'SJC' 'MORUMBI' 'LINS']\n\n--- Processing PAULINIA sheet ---\nConverted VALOR sample: [78661.77, 55947.52, 54432.92]\nNaN values after conversion: 10\nVALOR statistics: Min=6167.61, Max=21600212072624.0, Mean=1943374697564.2173\nUNIDADE values in PAULINIA: ['PAULINIA']\n\n--- Processing CAMPINAS sheet ---\nConverted VALOR sample: [47620.67, 51408.02, 13345.0]\nNaN values after conversion: 45\nVALOR statistics: Min=3681.72, Max=89595.06, Mean=36082.521073825505\nUNIDADE values in CAMPINAS: ['CAMPINAS']\n\n--- Summary of Data Analysis ---\nTotal sheets analyzed: 5\n\nAlphaville:\n  - Records: 1651\n  - Fields: 46\n  - Missing values: 9732\n  - Date range: 1899-11-30 00:00:00 to 2024-09-29 00:00:00\n\nBH:\n  - Records: 814\n  - Fields: 46\n  - Missing values: 2823\n\nSJC:\n  - Records: 2587\n  - Fields: 46\n  - Missing values: 15793\n  - Date range: 1899-11-25 00:00:00 to 2025-01-09 00:00:00\n\nPAULINIA:\n  - Records: 251\n  - Fields: 46\n  - Missing values: 605\n  - Date range: 1900-01-10 00:00:00 to 2025-01-07 00:00:00\n\nCAMPINAS:\n  - Records: 194\n  - Fields: 45\n  - Missing values: 821\n  - Date range: 1900-02-25 00:00:00 to 2024-12-31 00:00:00\n\n--- Structural Inconsistencies ---\nTotal unique columns across all sheets: 50\nCommon columns across all sheets: 43\nAlphaville has 3 unique columns: {'PACIENTYES', 'DATA PROTOCOLO ', 'Nº CONTRATO (SISTEMA GERA)'}\nBH has 3 unique columns: {'PACIENTE 2', 'Nº CONTRATO (SISTEMA GERA)2', 'DATA PROTOCOLO '}\nSJC has 3 unique columns: {'PACIENTE 2', 'Nº CONTRATO (SISTEMA GERA)', 'DATA PROTOCOLO '}\nPAULINIA has 3 unique columns: {'Nº CONTRATO (SISTEMA GERA)', 'DATA PROTOCOLO ', 'USUÁRIO'}\nCAMPINAS has 2 unique columns: {'Nº CONTRATO (SISTEMA GERA)', 'CLIENTES'}\n\nType inconsistencies in common columns:\nColumn 'VALOR FINANCIADO' has different types:\n  - float64: in sheets Alphaville\n  - object: in sheets BH, SJC, PAULINIA, CAMPINAS\nColumn 'COMPETÊNCIA' has different types:\n  - datetime64[ns]: in sheets Alphaville, SJC, PAULINIA, CAMPINAS\n  - float64: in sheets BH\nColumn 'DATA FINANCIAMENTO' has different types:\n  - object: in sheets Alphaville, SJC, CAMPINAS\n  - int64: in sheets BH\n  - datetime64[ns]: in sheets PAULINIA\nColumn 'CONTAS A RECEBER' has different types:\n  - object: in sheets Alphaville, SJC, CAMPINAS\n  - float64: in sheets BH, PAULINIA\nColumn 'DATA DO PAGAMENTO FACTORING' has different types:\n  - datetime64[ns]: in sheets Alphaville, SJC\n  - object: in sheets BH, PAULINIA, CAMPINAS\nColumn 'DATA DA RECLAMAÇÃO' has different types:\n  - datetime64[ns]: in sheets Alphaville, SJC, PAULINIA, CAMPINAS\n  - int64: in sheets BH\nColumn 'DATA DO PAGAMENTO REEMBOLSO' has different types:\n  - object: in sheets Alphaville, BH, SJC, PAULINIA\n  - datetime64[ns]: in sheets CAMPINAS\nColumn 'DATA PAGAMENTO CLÍNICA' has different types:\n  - datetime64[ns]: in sheets Alphaville, SJC, PAULINIA, CAMPINAS\n  - object: in sheets BH\nColumn ' NF' has different types:\n  - float64: in sheets Alphaville, CAMPINAS\n  - object: in sheets BH, SJC\n  - int64: in sheets PAULINIA\nColumn 'DATA DA NIP' has different types:\n  - datetime64[ns]: in sheets Alphaville, SJC, PAULINIA, CAMPINAS\n  - int64: in sheets BH\nColumn 'VALOR PAGAMENTO CLÍNICA' has different types:\n  - object: in sheets Alphaville, BH, CAMPINAS\n  - float64: in sheets SJC, PAULINIA\nColumn 'DATA DO CONTRATO' has different types:\n  - object: in sheets Alphaville, BH, SJC\n  - datetime64[ns]: in sheets PAULINIA, CAMPINAS\nColumn 'VALOR PAGAMENTO FACTORING' has different types:\n  - float64: in sheets Alphaville\n  - object: in sheets BH, SJC, PAULINIA, CAMPINAS\nColumn 'DATA DA LIGAÇÃO' has different types:\n  - datetime64[ns]: in sheets Alphaville, SJC, PAULINIA, CAMPINAS\n  - int64: in sheets BH\nColumn 'DATA DE VENCIMENTO' has different types:\n  - datetime64[ns]: in sheets Alphaville, SJC, CAMPINAS\n  - float64: in sheets BH\n  - object: in sheets PAULINIA\nColumn 'DATA EMISSÃO NF' has different types:\n  - datetime64[ns]: in sheets Alphaville, SJC, PAULINIA, CAMPINAS\n  - int64: in sheets BH\nStandardizing Alphaville sheet...\n  Processed 1651 rows with 47 columns\nStandardizing BH sheet...\n  Processed 814 rows with 47 columns\nStandardizing SJC sheet...\n  Processed 2587 rows with 47 columns\nStandardizing PAULINIA sheet...\n  Processed 251 rows with 47 columns\nStandardizing CAMPINAS sheet...\n  Processed 194 rows with 46 columns\n\nInconsistent client IDs (multiple names for the same ID):\n  Client ID 10 has 2 different names:\n    - PEDRO SILVA\n    - Miguel Oliveira Costa\n  Client ID 100 has 3 different names:\n    - PEDRO SILVA\n    - JOANA MARIA-1ª HM\n    - RUBENS FERREIRA-matmed\n  Client ID 1000 has 4 different names:\n    - Miguel costa\n    - Roberto Amorin-único\n    - Roberto Amorin-1ª HM\n    - Felipe Cardoso Nunes\n  Client ID 1001 has 2 different names:\n    - Roberto Amorin-1ª matmed\n    - Julio Freitas-1ª\n  Client ID 1004 has 2 different names:\n    - Roberto Amorin-1ª HM\n    - Julio Freitas-2º\n  Client ID 1008 has 2 different names:\n    - Roberto Amorin-0\n    - Julio Freitas-1ª HM\n  Client ID 1010 has 2 different names:\n    - Miguel costa\n    - Felipe Cardoso Nunes\n  Client ID 1012 has 2 different names:\n    - Roberto Amorin-1ª HM\n    - Julio Freitas-2º\n  Client ID 1016 has 2 different names:\n    - Roberto Amorin-0\n    - Julio Freitas-2º\n  Client ID 1020 has 3 different names:\n    - Miguel costa\n    - Julio Freitas-2º\n    - Felipe Cardoso Nunes\n  Client ID 1024 has 2 different names:\n    - Roberto Amorin-0\n    - Julio Freitas-2º\n  Client ID 1030 has 2 different names:\n    - Miguel costa\n    - Felipe Cardoso Nunes\n  Client ID 10300 has 2 different names:\n    - Amanda Lopes Ribeiro\n    - LaUrA nUnES rIbEirO\n  Client ID 10310 has 2 different names:\n    - Amanda Lopes Ribeiro\n    - LaUrA nUnES rIbEirO\n  Client ID 1032 has 2 different names:\n    - Roberto Amorin-0\n    - Julio Freitas-2º\n  Client ID 10320 has 2 different names:\n    - Amanda Lopes Ribeiro\n    - LaUrA nUnES rIbEirO\n  Client ID 10330 has 2 different names:\n    - Amanda Lopes Ribeiro\n    - LaUrA nUnES rIbEirO\n  Client ID 10340 has 2 different names:\n    - Amanda Lopes Ribeiro\n    - LaUrA nUnES rIbEirO\n  Client ID 10350 has 2 different names:\n    - Amanda Lopes Ribeiro\n    - LaUrA nUnES rIbEirO\n  Client ID 10360 has 2 different names:\n    - Amanda Lopes Ribeiro\n    - LaUrA nUnES rIbEirO\n  Client ID 10370 has 2 different names:\n    - Amanda Lopes Ribeiro\n    - LaUrA nUnES rIbEirO\n  Client ID 10380 has 2 different names:\n    - Amanda Lopes Ribeiro\n    - LaUrA nUnES rIbEirO\n  Client ID 104 has 2 different names:\n    - JOÃO GUIMARAES-único\n    - RUBENS FERREIRA-hdf\n  Client ID 1040 has 4 different names:\n    - Miguel costa\n    - Roberto Amorin-0\n    - Julio Freitas-2º\n    - Felipe Cardoso Nunes\n  Client ID 1041 has 2 different names:\n    - Julio Freitas-1ª\n    - Julio Freitas-único - hm\n  Client ID 1048 has 2 different names:\n    - Roberto Amorin-0\n    - Julio Freitas-2º\n  Client ID 105 has 4 different names:\n    - RUBENS FERREIRA-dp\n    - nan\n    - MaRcElO nuNeS FeRreiRa Mat e Med\n    - MaRcElO nuNeS FeRreiRa MAT & MED\n    - diEgO cArDoSo nuNeS mat e MED\n  Client ID 1050 has 2 different names:\n    - Miguel costa\n    - Felipe Cardoso Nunes\n  Client ID 1052 has 2 different names:\n    - Julio Freitas-2º\n    - Julio Freitas-hdf\n  Client ID 1056 has 2 different names:\n    - Roberto Amorin-0\n    - Julio Freitas-matmed\n  Client ID 1057 has 2 different names:\n    - Julio Freitas-único - hm\n    - Julio Freitas-1º\n  Client ID 1060 has 4 different names:\n    - Miguel costa\n    - Julio Freitas-hdf\n    - Julio Freitas-matmed\n    - Felipe Cardoso Nunes\n  Client ID 1064 has 2 different names:\n    - Julio Freitas-1ª\n    - Julio Freitas-hdf\n  Client ID 1065 has 2 different names:\n    - Julio Freitas-dp\n    - Julio Freitas-1º\n  Client ID 1068 has 2 different names:\n    - Julio Freitas-matmed\n    - Julio Freitas-1ª\n  Client ID 1070 has 2 different names:\n    - Miguel costa\n    - Felipe Cardoso Nunes\n  Client ID 1072 has 2 different names:\n    - Julio Freitas-1ª\n    - Julio Freitas-matmed\n  Client ID 1076 has 2 different names:\n    - Julio Freitas-hdf\n    - Julio Freitas-1ª\n  Client ID 10790 has 2 different names:\n    - Amanda Lopes Ribeiro\n    - LaUrA nUnES rIbEirO\n  Client ID 108 has 3 different names:\n    - JOANA MARIA-2º\n    - RUBENS FERREIRA-hdf\n    - Miguel Oliveira Costa\n  Client ID 1080 has 2 different names:\n    - Isabela lima\n    - Julio Freitas-1ª\n  Client ID 10800 has 2 different names:\n    - Amanda Lopes Ribeiro\n    - LaUrA nUnES rIbEirO\n  Client ID 10810 has 2 different names:\n    - Amanda Lopes Ribeiro\n    - LaUrA nUnES rIbEirO\n  Client ID 10820 has 2 different names:\n    - Amanda Lopes Ribeiro\n    - LaUrA nUnES rIbEirO\n  Client ID 10830 has 2 different names:\n    - Amanda Lopes Ribeiro\n    - LaUrA nUnES rIbEirO\n  Client ID 10840 has 2 different names:\n    - Amanda Lopes Ribeiro\n    - LaUrA nUnES rIbEirO\n  Client ID 10850 has 2 different names:\n    - Amanda Lopes Ribeiro\n    - LaUrA nUnES rIbEirO\n  Client ID 10860 has 2 different names:\n    - Amanda L\n=== KEY PORTFOLIO METRICS ===\nTotal portfolio value: R$ 0.00\nTotal number of contracts: 5497\nAverage contract value: R$ nan\n\nUnit Performance Summary:\n\nALPHAVILLE:\n  - Contracts: 1,652\n  - Total value: R$ 0.00\n  - Average value: R$ nan\n  - Average sessions: 14.9\n  - Main status: ATIVO\n  - Main operator: BRADESCO\n\nBH:\n  - Contracts: 807\n  - Total value: R$ 0.00\n  - Average value: R$ nan\n  - Average sessions: 12.6\n  - Main status: ATIVO\n  - Main operator: BRADESCO REEMBOLSO\n\nOSASCO:\n  - Contracts: 5\n  - Total value: R$ 0.00\n  - Average value: R$ nan\n  - Average sessions: 12.2\n  - Main status: ATIVO\n  - Main operator: AMIL\n\nBRASILIA:\n  - Contracts: 1\n  - Total value: R$ 0.00\n  - Average value: R$ nan\n  - Average sessions: 13.0\n  - Main status: ATIVO\n  - Main operator: AMIL\n\nDOMICILIAR:\n  - Contracts: 1,622\n  - Total value: R$ 0.00\n  - Average value: R$ nan\n  - Average sessions: 16.3\n  - Main status: ATIVO\n  - Main operator: BRADESCO\n\nSAO JOSE DOS CAMPOS:\n  - Contracts: 841\n  - Total value: R$ 0.00\n  - Average value: R$ nan\n  - Average sessions: 16.3\n  - Main status: ATIVO\n  - Main operator: SULAMERICA \n\nMORUMBI:\n  - Contracts: 59\n  - Total value: R$ 0.00\n  - Average value: R$ nan\n  - Average sessions: 16.0\n  - Main status: ATIVO\n  - Main operator: SULAMERICA \n\nLINS:\n  - Contracts: 65\n  - Total value: R$ 0.00\n  - Average value: R$ nan\n  - Average sessions: 16.5\n  - Main status: ATIVO\n  - Main operator: SULAMERICA \n\nPAULINIA:\n  - Contracts: 251\n  - Total value: R$ 0.00\n  - Average value: R$ nan\n  - Average sessions: 16.3\n  - Main status: ATIVO\n  - Main operator: VIVEST\n\nCAMPINAS:\n  - Contracts: 194\n  - Total value: R$ 0.00\n  - Average value: R$ nan\n  - Average sessions: 16.3\n  - Main status: ATIVO\n  - Main operator: SULAMERICA\n\nVisualization files saved to: /data/chats/igk4wd/workspace/vizualizacoes\nProcessed data saved to: /data/chats/igk4wd/workspace/processed_data.csv,,,\nProcessing Alphaville sheet...\n  - Sample original values: [41545.78, 54270.99, 43074.45]\n  - Sample converted values: [41545.78, 54270.99, 43074.45]\n  - NaN values after conversion: 21\n  - VALOR statistics: Min=3000.0, Max=124526.12, Mean=47585.68\nProcessing BH sheet...\n  - Sample original values: [51231.24, 47328.33, 57888.45]\n  - Sample converted values: [51231.24, 47328.33, 57888.45]\n  - NaN values after conversion: 52\n  - VALOR statistics: Min=8.0, Max=84354.34, Mean=28020.54\nProcessing SJC sheet...\n  - Sample original values: [datetime.datetime(2014, 1, 20, 9, 21, 36), datetime.datetime(2016, 5, 29, 18, 57, 36), datetime.datetime(2016, 8, 29, 9, 21, 36)]\n  - Sample converted values: [20140120092136.0, 20160529185736.0, 20160829092136.0]\n  - NaN values after conversion: 18\n  - VALOR statistics: Min=2600.0, Max=26030602074048.0, Mean=11384180980684.39\nProcessing PAULINIA sheet...\n  - Sample original values: [78661.77, 55947.52, 54432.92]\n  - Sample converted values: [78661.77, 55947.52, 54432.92]\n  - NaN values after conversion: 10\n  - VALOR statistics: Min=6167.61, Max=21600212072624.0, Mean=1943374697564.22\nProcessing CAMPINAS sheet...\n  - Sample original values: [47620.67, 51408.02, 13345]\n  - Sample converted values: [47620.67, 51408.02, 13345.0]\n  - NaN values after conversion: 45\n  - VALOR statistics: Min=3681.72, Max=89595.06, Mean=36082.52\n\n=== PORTFOLIO PERFORMANCE ANALYSIS ===\nTotal portfolio value: R$ 29,714,314,345,783,776.00\nTotal number of contracts: 5,497\nAverage contract value: R$ 5,553,039,496,502.29\n\n=== UNIT PERFORMANCE SUMMARY ===\n\nALPHAVILLE:\n  - Contracts: 1,652\n  - Total value: R$ 77,569,468.39\n  - Average value: R$ 47,559.45\n  - Average sessions: 14.9\n  - Main status: ATIVO\n  - Main operator: BRADESCO\n\nBH:\n  - Contracts: 807\n  - Total value: R$ 21,331,388.35\n  - Average value: R$ 28,178.85\n  - Average sessions: 12.6\n  - Main status: ATIVO\n  - Main operator: BRADESCO REEMBOLSO\n\nOSASCO:\n  - Contracts: 5\n  - Total value: R$ 15,450.00\n  - Average value: R$ 3,862.50\n  - Average sessions: 12.2\n  - Main status: ATIVO\n  - Main operator: AMIL\n\nBRASILIA:\n  - Contracts: 1\n  - Total value: R$ 0.00\n  - Average value: R$ nan\n  - Average sessions: 13.0\n  - Main status: ATIVO\n  - Main operator: AMIL\n\nDOMICILIAR:\n  - Contracts: 1,622\n  - Total value: R$ 24,989,226,432,150,044.00\n  - Average value: R$ 15,473,205,221,145.54\n  - Average sessions: 16.3\n  - Main status: ATIVO\n  - Main operator: BRADESCO\n\nSAO JOSE DOS CAMPOS:\n  - Contracts: 841\n  - Total value: R$ 2,001,415,196,885,614.75\n  - Average value: R$ 2,408,441,873,508.56\n  - Average sessions: 16.3\n  - Main status: ATIVO\n  - Main operator: SULAMERICA \n\nMORUMBI:\n  - Contracts: 59\n  - Total value: R$ 1,180,853,629,471,216.00\n  - Average value: R$ 20,359,545,335,710.62\n  - Average sessions: 16.0\n  - Main status: ATIVO\n  - Main operator: SULAMERICA \n\nLINS:\n  - Contracts: 65\n  - Total value: R$ 1,074,465,680,871,320.50\n  - Average value: R$ 16,530,241,244,174.16\n  - Average sessions: 16.5\n  - Main status: ATIVO\n  - Main operator: SULAMERICA \n\nPAULINIA:\n  - Contracts: 251\n  - Total value: R$ 468,353,302,112,976.38\n  - Average value: R$ 1,943,374,697,564.22\n  - Average sessions: 16.3\n  - Main status: ATIVO\n  - Main operator: VIVEST\n\nCAMPINAS:\n  - Contracts: 194\n  - Total value: R$ 5,376,295.64\n  - Average value: R$ 36,082.52\n  - Average sessions: 16.3\n  - Main status: ATIVO\n  - Main operator: SULAMERICA\n\nProcessed data saved to: /data/chats/igk4wd/workspace/processed_data.csv\nVisualization files saved to: /data/chats/igk4wd/workspace/visualizations\n\n=== DATA INCONSISTENCIES ===\n1. Inconsistent column naming across sheets\n2. Inconsistent UNIDADE names (e.g., 'ALPHAVILLE', 'alpha ')\n3. Multiple patient names assigned to the same client ID\n4. Inconsistent data types for common columns across sheets\n5. Unusual date values (dates before 2000 likely errors)\n6. Extreme values in VALOR column that required filtering\n7. Missing values in critical columns,,\n=== DATA CLEANING AND STANDARDIZATION ===\n\nProcessing Alphaville sheet...\n  - Original values (sample): [41545.78, 54270.99, 43074.45]\n  - Converted values (sample): [41545.78, 54270.99, 43074.45]\n  - NaN values after conversion: 21 out of 1651\n  - VALOR statistics: Min=3000.00, Max=124526.12, Mean=47585.68\n\nProcessing BH sheet...\n  - Original values (sample): [51231.24, 47328.33, 57888.45]\n  - Converted values (sample): [51231.24, 47328.33, 57888.45]\n  - NaN values after conversion: 52 out of 814\n  - VALOR statistics: Min=8.00, Max=84354.34, Mean=28020.54\n\nProcessing SJC sheet...\n  - Original values (sample): [datetime.datetime(2014, 1, 20, 9, 21, 36), datetime.datetime(2016, 5, 29, 18, 57, 36), datetime.datetime(2016, 8, 29, 9, 21, 36)]\n  - Converted values (sample): [nan, nan, nan]\n  - NaN values after conversion: 1458 out of 2587\n  - VALOR statistics: Min=2600.00, Max=158214.06, Mean=44010.50\n\nProcessing PAULINIA sheet...\n  - Original values (sample): [78661.77, 55947.52, 54432.92]\n  - Converted values (sample): [78661.77, 55947.52, 54432.92]\n  - NaN values after conversion: 33 out of 251\n  - VALOR statistics: Min=6167.61, Max=101950.24, Mean=53151.71\n\nProcessing CAMPINAS sheet...\n  - Original values (sample): [47620.67, 51408.02, 13345]\n  - Converted values (sample): [47620.67, 51408.02, 13345.0]\n  - NaN values after conversion: 45 out of 194\n  - VALOR statistics: Min=3681.72, Max=89595.06, Mean=36082.52\n\nRows with valid VALOR: 3888 out of 5497 total rows\n\n=== PORTFOLIO PERFORMANCE ANALYSIS ===\nTotal portfolio value: R$ 165,567,533.31\nTotal number of contracts: 5,497\nContracts with valid financial data: 3,888 (70.7%)\nAverage contract value: R$ 42,584.24\n\n=== UNIT PERFORMANCE SUMMARY ===\n\nALPHAVILLE:\n  - Contracts: 1,652\n  - Valid financial data: 1,631 (98.7%)\n  - Total value: R$ 77,569,468.39\n  - Average value: R$ 47,559.45\n  - Average sessions: 14.9\n  - Main status: ATIVO\n  - Main operator: BRADESCO\n\nBH:\n  - Contracts: 807\n  - Valid financial data: 757 (93.8%)\n  - Total value: R$ 21,331,388.35\n  - Average value: R$ 28,178.85\n  - Average sessions: 12.6\n  - Main status: ATIVO\n  - Main operator: BRADESCO REEMBOLSO\n\nOSASCO:\n  - Contracts: 5\n  - Valid financial data: 4 (80.0%)\n  - Total value: R$ 15,450.00\n  - Average value: R$ 3,862.50\n  - Average sessions: 12.2\n  - Main status: ATIVO\n  - Main operator: AMIL\n\nBRASILIA:\n  - Contracts: 1\n  - Valid financial data: 0 (0.0%)\n  - Total value: R$ 0.00\n  - Average value: R$ nan\n  - Average sessions: 13.0\n  - Main status: ATIVO\n  - Main operator: AMIL\n\nDOMICILIAR:\n  - Contracts: 1,622\n  - Valid financial data: 386 (23.8%)\n  - Total value: R$ 16,678,035.16\n  - Average value: R$ 43,207.34\n  - Average sessions: 16.3\n  - Main status: ATIVO\n  - Main operator: BRADESCO\n\nSAO JOSE DOS CAMPOS:\n  - Contracts: 841\n  - Valid financial data: 731 (86.9%)\n  - Total value: R$ 32,521,622.85\n  - Average value: R$ 44,489.22\n  - Average sessions: 16.3\n  - Main status: ATIVO\n  - Main operator: SULAMERICA \n\nMORUMBI:\n  - Contracts: 59\n  - Valid financial data: 0 (0.0%)\n  - Total value: R$ 0.00\n  - Average value: R$ nan\n  - Average sessions: 16.0\n  - Main status: ATIVO\n  - Main operator: SULAMERICA \n\nLINS:\n  - Contracts: 65\n  - Valid financial data: 12 (18.5%)\n  - Total value: R$ 488,200.57\n  - Average value: R$ 40,683.38\n  - Average sessions: 16.5\n  - Main status: ATIVO\n  - Main operator: SULAMERICA \n\nPAULINIA:\n  - Contracts: 251\n  - Valid financial data: 218 (86.9%)\n  - Total value: R$ 11,587,072.35\n  - Average value: R$ 53,151.71\n  - Average sessions: 16.3\n  - Main status: ATIVO\n  - Main operator: VIVEST\n\nCAMPINAS:\n  - Contracts: 194\n  - Valid financial data: 149 (76.8%)\n  - Total value: R$ 5,376,295.64\n  - Average value: R$ 36,082.52\n  - Average sessions: 16.3\n  - Main status: ATIVO\n  - Main operator: SULAMERICA\n\nDetailed report saved to: /data/chats/igk4wd/workspace/visualizations/portfolio_analysis_report.md\nProcessed data saved to: /data/chats/igk4wd/workspace/processed_data.csv\nVisualization files saved to: /data/chats/igk4wd/workspace/visualizations,,",
        "is_success": true,
        "is_finished": true,
        "assignee": "David",
        "start_at": 1748200192.9464848,
        "end_at": 1748200733.116703
      },
      "current_task_id": "1"
    }
  },
  "gallery_last_file": {},
  "timeline_status": {
    "1": 1,
    "2": 1,
    "3": 1,
    "4": 1,
    "5": 1,
    "6": 1,
    "7": 1,
    "8": 1,
    "9": 1,
    "10": 1,
    "11": 1,
    "12": 1,
    "13": 1,
    "14": 1,
    "15": 1,
    "16": 1,
    "17": 1,
    "18": 1,
    "20": 0,
    "21": 1,
    "22": 0,
    "23": 1
  },
  "preview_servers": {
    "bd51be58-3889-450f-9b92-ff5280109f4b": {
      "role": "Alex",
      "port": 8501,
      "subdomain": "deveras-portfolio-dashboard-4gv9y-igk4wd-109f4b",
      "server": "049b91d7-51c7-4f1a-b3e9-d8f5f9234b88",
      "cmd": "cd /data/chats/igk4wd/workspace/streamlit_template && pip install -r requirements.txt && streamlit run app.py",
      "timeline_index": 22,
      "status": "success",
      "logs": []
    }
  },
  "deploy": {}
}